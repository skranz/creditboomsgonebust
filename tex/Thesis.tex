%License: Creative Commons (CC BY-NC-ND)
\documentclass[a4paper,11pt,abstract=on]{scrartcl}
\addtokomafont{sectioning}{\rmfamily}
\usepackage{titlesec}

\titleformat*{\section}{\LARGE\bfseries}
\titleformat*{\subsection}{\LARGE}
\titleformat*{\subsubsection}{\Large}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{amssymb,amsmath,bm}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded2}{\begin{snugshade}\small}{\end{snugshade}}
\newenvironment{Shaded}{\begin{snugshade}\small}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textbf{{#1}}}
\newcommand{\NormalTok}[1]{{#1}}
\newcommand{\IN}[1]{\rule{#1em}{0em}}
\usepackage[framemethod=tikz]{mdframed}

% normale infobox ohne verbatim
\usepackage[framemethod=tikz]{mdframed}
\newcommand{\infobox}[2]{\begin{mdframed}[linewidth=0.8pt, innerleftmargin = 0.8cm, innerrightmargin = 0.8cm, innertopmargin = 0.8cm, innerbottommargin = 0.8cm,skipabove=0.5cm,skipbelow=1.5cm]{
\sf{\textbf{#1} }\\  #2 
}\end{mdframed}}


% verbatim work around
\newenvironment{infobox2}{}{}
\newenvironment{SAMEPAGE}{}{}

\newcommand{\COMMENT}[1]{{\fboxrule0pt \vskip 1em \hrule \fbox{
\begin{minipage}{0.93\textwidth}
{\rm {\bf Comment\\[0.5em]}#1}
\end{minipage}
}\vskip 0.5em \hrule \vskip 1em}}



\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{1}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}


%%% Change title format to be more compact
\usepackage{titling}
\setlength{\droptitle}{-2em}
  \title{}
  \pretitle{\vspace{\droptitle}}
  \posttitle{}
  \author{}
  \preauthor{}\postauthor{}
  \date{}
  \predate{}\postdate{}

% hier Namen etc. einsetzen
\newcommand{\titel}{Credit Growth and Financial Crises: \\An Interactive Problem Set in R}
\newcommand{\type}{Abschlussarbeit}
\newcommand{\fullname}{Thomas Clausing}
\newcommand{\email}{thomas.clausing@uni-ulm.de}
\newcommand{\datum}{22.04.2015}
\newcommand{\matnr}{681931}
\newcommand{\gutachterA}{Prof.~Dr.~Sebastian Kranz}
\newcommand{\gutachterB}{Prof.~Dr.~Georg Gebhardt}
\newcommand{\fakultaet}{Mathematik und Wirtschaftswissenschaften}
\newcommand{\institut}{Institut für Wirtschaftswissenschaften}
\newcommand{\studiengang}{Wirtschaftsmathematik}


\usepackage{float}
\linespread{1.15}

\surroundwithmdframed[linewidth=0.8pt, innerleftmargin = 0.8cm, innerrightmargin = 0.8cm,
 innertopmargin = 0.8cm, innerbottommargin = 0.8cm,skipabove=0.5cm,skipbelow=1.5cm]{infobox2}
 
\surroundwithmdframed[linewidth=0pt, innerleftmargin = 0cm, innerrightmargin = 0cm,
 innertopmargin = 0cm, innerbottommargin = 0cm,skipabove=0.5,skipbelow=1.5cm, nobreak=true]{SAMEPAGE}


\begin{document}
\begin{titlepage}
\textwidth28cm
\includegraphics[height=1.8cm]{images/unilogo_bild}
\hfill
\includegraphics[height=1.8cm]{images/unilogo_wort}\\[1em]
\begin{center} 
{\large\bfseries Universität Ulm\\
	\vspace{0.5cm}
   Fakultät für \fakultaet\\
}
\vspace{0.5cm}
{\large \institut\\
}
%\vspace{1cm}
\begin{figure}[h]
\centering

\end{figure}
\vspace{3cm}
{\LARGE\bfseries
\titel\\
}
\vspace{1cm}
{\Large
\type}\\
\mbox{}\\
\mbox{in \studiengang}
\end{center}
\vspace{3cm}
\begin{center}
vorgelegt von \\
\mbox{\fullname} \\
\mbox{(Matrikelnummer: \matnr)} \\
am \mbox{\datum}\\

\vspace*{1.2cm}
{\bf Gutachter} \\
\mbox{}\\
\mbox{\gutachterA}\\
\mbox{\gutachterB}\\
\end{center}
\vfill
\end{titlepage}

\newpage\thispagestyle{empty}


\maketitle

\begin{abstract}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\parindent}{0pt}
\noindent
This thesis presents an interactive problem set based on the paper
"Credit booms gone bust: Monetary Policy, Leverage Cycles, and Financial Crises, 1870 -- 2008" by Schularick and Taylor (2012). 

The findings of this paper were obtained by computations programmed
in the statistical computing language Stata. The underlying data set
and the Stata code are made public by the authors.

The main aim of this thesis is to reproduce the results of the
paper as reliably as possible in the R language, to explain the
reasoning and the calculations of Schularick and Taylor in detail,
and thus make their paper more accessible.

The problem set developed here is written in R using the RTutor
environment created and maintained by S. Kranz.
\eject
\end{abstract}

{
\linespread{1.5}
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{3}
\tableofcontents
}

\eject

\section{Introduction}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
The recent financial crisis which began in 2007  has caused repercussions on the world economy lasting until today. Anomalies like very low and sometimes even negative interest rates in the money market and a flooding of the economies with money as it is presently done by leading central banks to stimulate the growth of gross domestic products are seen by many (e.g. (Maddaloni \& Peydro, 2011), (Starr, 2011), (Dell'Ariccia, Laeven, \& Marquez, 2014)) as a cause for a new credit boom which eventually may lead to the next financial crisis.

The fact that the crisis of 2007 has not been foreseen, at least not in this severity and certainly not by the main economic actors, has drawn the focus of many economists to the question of its causes. One way to address this question is to have a closer look at economic history.
By investigating financial indicators of the economies of many countries over a long period of time, one may hope to get better insight into the complicated reasons of financial crises, depressions, shocks, and related macroeconomic events. The importance of studying money and credit data on a broad scale have recently been stressed by M. Schularick and A. M. Taylor in their paper "Credit booms gone bust: Monetary Policy, Leverage Cycles, and Financial Crises, 1870 -- 2008" which was published in the American Economics Journal (Schularick \& Taylor, 2012).

They have collected a very comprehensive data set for 14 developed countries over a time frame of 139 years, containing yearly data for aggregate bank loans and total bank assets. To these they have added data on money and other economic aggregates like price indices, gross domestic product, and investment data. The comprehensiveness of this approach is a novel feature of Schularick and Taylor’s article: “The contribution of this paper is to make a start on the broader, systematic, cross-country quantitative history of money and credit” (p. 1031).

Schularick and Taylor have paid special attention to three questions.

\begin{enumerate}
\item	Do the data reveal any striking long term trends? One outstanding insight which is convincingly presented is a dichotomy of financial trends between two eras, before and after the Second World War.
\item	Based on the findings as to question 1), how and why did the impacts of crises on economic growth, investment growth, and credit growth change over time? For example, it appears from the data that the effects of a crisis on bank loans have significantly decreased from the first era to the second, whereas for the effects on total bank assets the reverse is true.
\item	Can credit growth be used as a predictive measure to forecast future crises and is this forecast robust with respect to other influences? Throughout the investigations, credit growth has proved to be able to predict crises to a certain extent, at least better than other indicators.
\end{enumerate}

Schularick and Taylor have analysed their data using code written in the Stata statistical language. Their code has been made publicly available and thus can be controlled by an interested reader. However, Stata is a proprietary language which may keep readers not having access to this language from being able to familiarize himself too closely with the results.

A better way to approach the paper would be a ‘guided tour’ through its main points which at the same time allows the reader to recreate and vary the computations without running into the problems of proprietary software. 

The main purpose of the present thesis is to provide the reader with an opportunity to do that. In order to achieve this, two things were needed: 

\begin{itemize}
\item A complete reprogramming of the Stata code in the open source statistical language R, and 
\item an incorporation into an interactive problem set as offered by the RTutor environment created by S. Kranz (Kranz, 2014). 
\end{itemize}

A secondary purpose of such an interactive problem set is to strengthen the reader’s R programming and statistical data manipulation skills. We expect the user, however, to have at least a rudimentary knowledge of~R.

The replication of the Stata code in R has the additional benefit of a critical code revision. Great emphasis has been given to replicating the results as accurately as possible. In the course of this undertaking several inaccuracies and at least one serious mistake have been found. The reader is invited to also look critically at the presented R code. The openness of the RTutor framework supports this. We also advise the user of the problem set to have the original paper at hand since we have not attempted to rephrase its content completely\footnote{The paper can be found under the link \url{https://www.aeaweb.org/articles.php?doi=10.1257/aer.102.2.1029}}. The focus has been on the three questions pointed out above, they correspond essentially with the three exercise~sets~1~–~3 of the interactive problem set.

The problem set can be found at \url{https://tcl89.shinyapps.io/creditboomsgonebust}. Within this thesis the problem set is discernible by a {\sf sans serif font family}.

Finally, I would like to express my thanks to Prof. Sebastian Kranz for providing me with a challenging topic and for his support and constant willingness to help.

\eject

{\sffamily
\section{Interactive Problem Set to ``Credit Booms Gone
Bust''}\label{interactive-problem-set-to-credit-booms-gone-bust}

Name: Jane Doe

Author: Thomas Clausing

Date: 22/04/2015\\

In the paper ``Credit booms gone bust: Monetary Policy, Leverage Cycles,
and Financial Crises, 1870 -- 2008'' by M. Schularick and A. M. Taylor,
the authors take the financial crisis of~\mbox{2007~--~2009} as a cause to
investigate the role of money and credit fluctuations and in particular
credit booms in generating financial instabilities and crashs.

In this problem set you will be guided through Schularick and Taylor's main findings. Besides, you will learn how to deal with advanced panel data in R.

\subsection{\sf Exercise Introduction}\label{exercise-overview}
In this problem set, you find three sets of exercises. Each of these is separated into smaller parts. At the beginning of each set, you will find a brief description of its respective parts.

The overall structure is as follows:

In \textbf{Exercise 1}, a marked structural difference in the development of important financial indicators for the times before and after the Second World War gets verified.

\textbf{Exercise 2} discusses the way these and other economic indicators are affected by a financial crisis in the subsequent years.

Finally, in \textbf{Exercise 3} the focus is on the question whether it is possible to extract from the data a means for predicting future crises.
\begin{spacing}{1.3}
You don't have to be an expert in R although some R and programming
knowledge in general will certainly help you. Nevertheless, you will
usually be given examples of code and will then be given instructions to
solve similar problems yourself. Where the instructions are insufficient
for you to solve the code, feel free to use the \texttt{hint} button,
which is available for each code chunk, to get additional help.
Furthermore you can use the \texttt{solution} button if you get stuck to
see the solution. Pay attention to the comments within the code chunks,
which are also there to help you. Apart from the two mentioned buttons,
you will be able to use a \texttt{data} button to view the data used in
a code chunk. Additionally, you will be given lots of short extracts of
the objects you are using and creating, which will hopefully help you to
keep track of the data.

Sometimes you will be given additional code chunks in info blocks which
are supposed to demonstrate a matter described in the info block. If you
skip a block with a code chunk inside, that is totally okay. In this
case you will have to click on the \texttt{edit} button before you can
input code into the subsequent code chunk. Moreover, you can run parts
of your code by selecting it and then holding down the \texttt{Ctrl} and
the \texttt{Enter} keys of your keyboard at the same time. To run the
entire code at once instead, you can click the \texttt{run chunk} button
alternatively. Finally, when you are done with the coding click
\texttt{check} to get your code get checked. If it is not correct,
consider reading the instructions again and/or try the \texttt{hint}
button.
\end{spacing}
Although not mandatory, it is recommended to solve the exercise sets in the given order. Within
the introductions to each exercise set you will be given additional 
advice for further steps.

\eject

\subsection{\sf Exercise 1 -- Two Eras of Financial Development}\label{exercise-1-overview}

For the purposes of their paper, Schularick and Taylor (2012a) collected
data and provided a data set containing different aggregates. In
exercise 1 the relationship between three variables from this data set
will be evaluated. The focus will lay on the series of total loans,
total assets and broad money. The authors suggest that their data set
provides evidence of two distinct eras with profoundly different
economic and financial dynamics. In order to reconstruct these
results, they advise to create quotients of some of the variables. Being
compared between the two eras, these quotients and their global averages
show, according to the authors, dissimilar behaviour, which therefore is
the reason for Schularick and Taylor to talk about the different eras.

Exercise 1 is divided into three major parts.

Exercise 1.1 serves as a quick introduction. You will have to create a
plot showing the relationship of total assets and money that slightly
differs compared to the original plot from Schularick and Taylor
(2012a). For the economic assertion of the existence of two eras in
the relationship of broad money and total assets, this plot is more or
less sufficient.

In exercise 1.2 you will then have to use the approach suggested by
Schularick and Taylor in order to reproduce their results exactly.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  In part a), you will find detailed theoretical information on the
  proposed fixed effects regression for the calculation of the global
  averages and also on the differences in the way of how the method is
  implemented in Stata and in R.
\item
  In part b), you are then taught how to replicate the exact results of
  the analysis of the relationship of total loans against broad money.
\item
  With this knowledge you can then attempt part c), where you are guided
  through the computation of the global averages of total assets and
  money again, but this time by the fixed effects method. It will turn
  out that in comparison to the results from exercise 1.1 the
  differences are very small.
\end{itemize}

In exercise 1.3 we will finally merge the results from the first parts
of this exercise and are going to add some more economic
interpretation. Thus if you are only interested in these final results,
you may skip the first parts of this exercise and proceed directly with
1.3.

\subsubsection{\sf Exercise 1.1 -- First Evidence of Two Eras}\label{exercise-1ux5f1-credit-and-money-first-evidence-of-two-eras}

In this part of the exercise you are going to reproduce a plot of global
averages over time for the quotient of total assets by money.

For a start,you just have to do the averaging. You don't have to adjust
or to create any new variables and the quotients of total assets by
money has been already prepared for you. Furthermore, you are going to
use an easier approach for the calculation than the one in Schularick
and Taylor's paper. (In the second part of the exercise, we are also
going to learn their way of calculating the averages and we will have a
look into the exact differences of both methods.)

We start with loading the prepared data file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_prep =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data_prep.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When you are dealing with a data set for the first time, it is always a
good idea to start by making yourself familiar with it. One way to do so
is passing the data object to the function \emph{str()}, so start with
that for now.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data_prep)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## 'data.frame':    1946 obs. of  3 variables:
##  $ iso             : Factor w/ 14 levels "AUS","CAN","CHE",..: 1 1 1 1 1 1 1 ...
##  $ year            : int  1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 ...
##  $ logAssetsbyMoney: num  NA NA NA 0.104 0.109 ...
\end{verbatim}
}

In the first row of this output, you can find some details about your
data object in general. It is of class \texttt{data.frame} and it
consists of three variables (columns) and 1946 observations (rows). In
the next few rows of the output, you can see a little description of
each variable in the data.frame: Your first variable has the class
\texttt{factor} and comprises 14 levels (categories). The second one is
of class \texttt{integer} and you can see the first few elements of this
variable. The third variable is of class \texttt{numeric}. Here we could
theoretically see the first few values of this variable as well, but
three of them are NA, we lack a value at this point.

\infobox{Info: Classes}{

\emph{str()} and \emph{summary()} are functions which are able to handle
many different kinds of objects with different properties. Such
functions are called generic or overloaded functions.

For these generic functions to be able to distinguish between the
different types of objects they rely on the class-attribute of the
objects. Based on this attribute, the function recognises the object's
properties and can therefore determine the way in which the object is
being handled

A class is a concept of object oriented programming which basically
groups objects according to their properties. If you look into the R
language definition, you will find it defined as: ``A class is a
definition of an object. Typically a class contains several slots that
are used to hold class-specific information. An object in the language
must be an instance of some class.'' (R~Core~Team,~2015,~p.~31).

In this problem set we are going to use generic functions throughout the
exercises. To get the desired output from the functions, it will be
quite important that the argument variables have the right classes. For
this reason you will often see checks and conversions of objects into
objects of different types in later tasks.
}

Another convenient way to get a first impression of the data is provided
by the \emph{summary()} function. Compared to \emph{str()}, it is more
descriptive about the content of the object, but is not as precise about
its structure. As with \emph{str()}, you only have to pass your data set
as arguments to the function \emph{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(data_prep)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##       iso            year      logAssetsbyMoney 
##  AUS    : 139   Min.   :1870   Min.   :-1.7773  
##  CAN    : 139   1st Qu.:1904   1st Qu.:-0.0398  
##  CHE    : 139   Median :1939   Median : 0.2431  
##  DEU    : 139   Mean   :1939   Mean   : 0.2293  
##  DNK    : 139   3rd Qu.:1974   3rd Qu.: 0.5831  
##  ESP    : 139   Max.   :2008   Max.   : 1.4392  
##  (Other):1112                  NA's   :421
\end{verbatim}
}

In the top row you can see the names of each column in
\textbf{data\_prep}. From the first column we can deduct that it
contains a variable called \textbf{iso}. It contains the elements
``AUS'', ``CAN'', ``CHE'', ``DEU'', ``DNK'', ``ESP'', each of these
elements appears 139 times. From the last row we see that there are more
elements, and all other elements together appear 1112 times in the data
set. The second and third column look slightly different. In the first
row we find the name of the variable, and of special interest for us are
the presented maximum and minimum values. This implies that the data set
comprises observations from 1870 until 2008. In the third column we find
the series of values that we are particularly interested in for now,
namely the values of the first quotient of two aggregates from the
original data set. As the name \textbf{logAssetsbyMoney} indicates, the
values of the quotients of total assets / broad money are logarithmized.
This is useful because the underlying data series have exponential
growth and the logarithmized form displays this linearly, which can be
handled by linear regression and also is easier to detect visually. We
see that the values range from -1.77 to +1.44. From the science of the
logarithm we can deduce that at some points in time for some countries
total assets had a larger amount than broad money, but at some other
points in time, broad money was actually higher than total assets. The
mean value 0.23 suggests that on average total assets were a bit higher
than broad money.

\infobox{Info: Data frame tbl}{

So far, we have tried to get an impression of the data set by using
functions which describe the data. Of course, you can also always just
display the whole object by simply entering the object's name. This is
not always a good idea, though. If the object is very long for example,
as it is the case with many data in this problem set, this will
basically flood your display. The good news are, that there are
different ways to prevent this. One way is using the function
\emph{head()}, which you will get to know later on.

Another possibility is a concept introduced by the package
\texttt{dplyr} that prevents your display from being flooded from the
output. The package adds a functionality to wrap a data frame (the data
type that we are using for data sets throughout this problem set), just
changing its printing behaviour in the way to print only ten
observations of as many columns as fit on the screen. Apart from that,
it behaves like a normal data frame, so you can use it just use it like
a `normal' data frame.

In the next task, the data set at hand is going to be grouped by one of
its variables. Whenever an object is grouped with the function
\emph{group\_by()} from the package \texttt{dplyr}, it will be returned
as data frame tbl, so keep an eye on the returned object to get a first
practical impression of its behaviour.
}

Now that we know the basics and the structure of the data set, we can
calculate the global averages for the variable \textbf{logAssetsbyMoney}
in two steps: In the first step, use the function \emph{group\_by()}
from the package \texttt{dplyr} to group the data set
(\textbf{data\_prep}) by year. Assign the return value to the variable
\textbf{dp\_byyear}.

Following the grouping, you should have a look at \textbf{dp\_byyear}.
You are given a hand with this task. Just fill in the missing values
indicated by `?' and uncomment the lines. If you are struggling with
this, you might want to click on \texttt{hint} to get additional
instructions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we have to load the required package for the grouping functionality}
\KeywordTok{library}\NormalTok{(dplyr) }
\CommentTok{# Uncomment the next line and adapt the correct parameters to call the function}
\CommentTok{# dp_byyear = group_by(?, ?)}
\NormalTok{dp_byyear =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(data_prep, year)}
\NormalTok{dp_byyear}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [1,946 x 3]
## Groups: year
## 
##    iso year logAssetsbyMoney
## 1  AUS 1870               NA
## 2  AUS 1871               NA
## 3  AUS 1872               NA
## 4  AUS 1873        0.1036875
## 5  AUS 1874        0.1085659
## 6  AUS 1875        0.1004536
## 7  AUS 1876        0.1017611
## 8  AUS 1877        0.1162472
## 9  AUS 1878        0.1449072
## 10 AUS 1879        0.1263519
## .. ...  ...              ...
\end{verbatim}
}

See how only the first ten observations are displayed? This is the
behaviour described in the previous info-block.

So far, the output does not add much information to what you already
knew about the data, but it shows us that the variable \textbf{year}
serves as grouping variable. Internally the call to the function
\emph{group\_by()} has changed even more in the underlying structure of
the data. It has prepared it to be evaluated per defined groups by
function \emph{summarise()}, which also belongs to package
\texttt{dplyr}. You have to pass at least two arguments to this
function, too. The first one should contain the grouped data object.
Then you can pass an arbitrary number of additional parameters to
\emph{summarise()}, each one containing another function call on a
determinated variable within the grouped data frame. In this case, we
are only interested to calculate the means per group of the quotients,
so we only pass one more parameter to \emph{summarise()}.

After that, call your argument to display it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculation of means in groups in first parameter. 'na.rm = TRUE' removes missing }
\CommentTok{# observations;}
\CommentTok{# DlAM_mean = summarise( ? , global_average = mean( ? , na.rm = TRUE)) }
\CommentTok{# DlAM_mean}
\NormalTok{DlAM_mean =}\StringTok{ }\KeywordTok{summarise}\NormalTok{(dp_byyear, }\DataTypeTok{global_average =} \KeywordTok{mean}\NormalTok{(logAssetsbyMoney, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{DlAM_mean}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [139 x 2]
## 
##    year global_average
## 1  1870            NaN
## 2  1871      0.2159793
## 3  1872      0.2684652
## 4  1873      0.2400191
## 5  1874      0.2739449
## 6  1875      0.2972301
## 7  1876      0.2907255
## 8  1877      0.3055815
## 9  1878      0.3189643
## 10 1879      0.2827236
## ..  ...            ...
\end{verbatim}
}

Other than the grouped data object we looked at before, the result we
got from the function \emph{summarise()} contains two columns. The first
column contains the years which were used for grouping, and the second
column contains the mean values of variable \textbf{logAssetsbyMoney}
for each group, which in this case means for each year. Naturally,
variable \textbf{iso} has vanished in the course of the creation of the
averages.

\COMMENT{
The next info block serves well as example for two things:\\

First, we have decided that some additional analyses of the data 
might be interesting for the user to get a better feeling for the data
although they are not necessary to solve the problem set. Hence we have 
included them in additional info blocks.\\

Second, the output of the data in the table has been shortened in this
thesis but will of course be visible to the user of the problem set.
}

\begin{infobox2}%
\textbf{Info: Why the average in 1870 is NaN}

Although we have used the option \texttt{na.rm = TRUE}, which removes
missing values from the calculation of the mean, the average for 1870
was returned as NaN. The reason for this is that all values of
observations from 1870, irrespectively for which country, are missing in
\textbf{data\_prep}. We can check that easily with the
function \emph{filter()} from the package \texttt{dplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show that all values for 1870 are NA, thus not existent for this variable}
\KeywordTok{filter}\NormalTok{(data_prep, year ==}\StringTok{ }\DecValTok{1870}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{SAMEPAGE}
{\small
\begin{verbatim}
##    iso year logAssetsbyMoney
## 1  AUS 1870               NA
## 2  CAN 1870               NA
## 3  CHE 1870               NA

   ¦    ¦    ¦                ¦  

## 12 NOR 1870               NA
## 13 SWE 1870               NA
## 14 USA 1870               NA
\end{verbatim}
}
\end{SAMEPAGE}
\end{infobox2}




In the next code chunk, you are supposed to create your first plot.
Plotting in R can be done using functions from the package
\texttt{ggplot2}, thus first we need to load this package (the code for
loading the package has been prepared for you). For easy plots, we can
use the function \emph{qplot()} (in later tasks we will be mostly using
\emph{ggplot()} instead). Use it, to plot global averages against years,
which are both saved in your data object \textbf{DlAM\_mean}. As before,
you receive a little help for this task.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# uncomment the next line and replace the '?' by the correct objects / variables}
\CommentTok{# qplot(x = ? , y = ? , data = ?)}
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \NormalTok{year, }\DataTypeTok{y =} \NormalTok{global_average, }\DataTypeTok{data =} \NormalTok{DlAM_mean)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/1_1__7-1.pdf}
\end{figure}

Now look at the plot you have just created. Between 1870 and the
beginning of the Second World War, the level of the graph remains fairly
stable, whereas we see that afterwards it drops down during the Second
World War, then it recovers quickly and reaches the pre war level after
about 20 years. Instead of remaining at that level, however, it actually
keeps increasing roughly at the same rate until today.

Since the plot displays the ratio of total assets by broad money, we can
deduce that the relationship between money and assets was pretty stable
before World War 2, which means in relative terms they grew at the same
rate at a global level. After World War 2 this relationship changed, and
total assets seems to have started growing stronger than broad money.

\subsubsection{\sf Exercise 1.2 a) -- Fixed Effects Regression in Stata and in R}\label{exercise-1ux5f2-a-some-theory}

To support their theory of two eras, Schularick and Taylor calculated
the global averages in a somewhat different way. They used the following
fixed effects regression model: \[y_{it} = a x_i + b x_t + e_{it}.\]
$x_i$ denotes the country, $x_t$ the year, $y_it$ is the variable for
which this regression calculates the averages and $e_{it}$ the error of
the regression. Note that both $x_i$ and $x_t$ are categorical
variables, which means we do not regress over a continuous explanatory
variable at all in this scenario. This might strike odd at first sight.
The way in which it is done, however, allows to calculate average values
over time of all countries, similar as this was done in exercise 1.1.

A fixed effects regression is basically a special case of a regression
with categorical predictors. Typically, categorical predictors are used
as an extension of an ordinary (possibly multiple) linear regression.
They are useful when the dependent variable is not only affected by the
continuous explanatory variables, but also from whether these
explanatory variables belong to a certain category which affects the
dependent variable's level. It can be done by employing dummies for each
category. The dummies' coefficients then reveal whether there is
actually statistical evidence for an influence from a category or not.

Instead of employing dummies as regressors, we can also conduct a fixed
effects regression. It's estimation is a bit more efficient, though one
can achieve the same results with a dummy regression, too.

\infobox{Info: Fixed effects estimation versus dummy
regression}{

A fixed effects estimation is usually taken into consideration when one
is interested in removing unobserved time-constant explanatory
variables. Although that could be achieved with a dummy regression as
well, it is more common to use the fixed effects estimation approach
since it is more efficient than a dummy regression, as you will see now.

In the following model, $i$ and $t$ will denote for the panel's index
variables, entity and time. $x_{it}$ is a single continuous explanatory
variable, $y_{it}$ the dependent variable, and $e_{it}$ an error term
that captures how much the observed value differs from the unobservable
true function value, often referred to as the idiosyncratic error. These
variables are all, as the subscript reveals, dependent on both index
variables. Contrary to that, $a_i$ is constant over time and thus only
depending on the respective entity. That means that all time-constant
effects on $y_{it}$ for each entity will be found in this variable, even
those, which we do not specifically control for. $a_i$ is therefore
often called unobserved heterogeneity or a fixed effect and is
name-giving for this kind of regression, since the fixed effects play a
key role in calculating the estimator, as we will see shortly.

For $t = \{1,2,...,T\}$ being the points in the given time frame,
$i = \{i_1,...,i_N\}$ being the entities, and $k = \{1,2,...,K\}$ with
$K$ being the number of explanatory variables, consider
\[ y_{it} = \beta_1 x_{it1} + ... +  \beta_K x_{itK}  + a_i + e_{it} .\]

Now we can take the mean over time which will leave us with:
\[\bar{y}_i = \beta_1 \bar{x}_{i1} + ... + \beta_K \bar{x}_{iK} + a_i + \bar{e}_i,\]

with $\bar{x}_{ik}$, $\bar{y}_i$, and $\bar{e}_i$ being the averages of
$x_{itk}$,$y_{it}$,and $e_{it}$. Note that $a_i$ did not change in the
second equation because it is constant over time. Hence we can remove
$a_i$ by deducting the first equation from the second one. As a
consequence we get
\[y_{it} - \bar{y}_i = \beta_1(x_{it1}-\bar{x}_{i1}) + ... + \beta_1(x_{itK}-\bar{x}_{iK})  + e_{it}- \bar{e}_i.\]

We have seen before that we cannot just plug categorical variables into
a regression, but that we need to convert them into many separate dummy
variables for each category, leaving us with N additional variables to
estimate. The fixed effects regression comes handy at this point,
because by getting rid of $a_i$ it makes that transformation into dummy
variables unnecessary and saves the estimation of these numerous dummy
coefficients.

For simplicity, we will substitute

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $\ddot{y}_{it} = y_{it} - \bar{y}_i$,
\item
  $\ddot{x}_{itk} = x_{itk} - \bar{x}_{ik}$, and
\item
  $\ddot{e}_{it} = e_{it} - \bar{e}_i$,
\end{itemize}

which then leaves us with
\[\ddot{y}_{it} = \beta_1\ddot{x}_{it1}+...+ \beta_K\ddot{x}_{itK} +\ddot{e}_{it}.\]

This last equation is estimated by a pooled ordinary least squares
regression (OLS). It will give us the same coefficients $\beta_k$ that
we would have gotten from a dummy regression on the original data, but
without estimating the fixed effects $a_i$. It therefore reduces the
computational costs to estimate the coefficients, although one can argue
that with computers being as fast as today, the advantage is rather
marginal. 

If we would carry out a fixed effects regression and then realize that
after all we were interested in the $a_i$'s, we could still calculate
their estimates. To do so, we need to plug in the estimates for the
$\beta_k$'s into the averaged initial formula, which gets us
$\bar{y}_i = \hat{\beta}_1 \bar{x}_{i1} + ... + \hat{\beta}_K \bar{x}_{iK} + \hat{a_i}$
(the error term disappears because of the underlying assumptions of the
OLS). This can then easily be rearranged to
\[\hat{a_i} =  \bar{y}_i - \hat{\beta}_1 \bar{x}_{i1} - ... - \hat{\beta}_K \bar{x}_{iK},\]
which allows us to calculate the fixed effects in a straightforward way.

Another thing we have to take care of, is that where the dummy
regression reports the correct degrees of freedom, the fixed effects
regression does not. When we do fixed effects regressions by hand, we
have to correct for the degrees of freedom manually. Since we have $N$
entities and $T$ points in time, we have $N \cdot T$ total observations.
For $K$ explanatory variables that would leave us with $NT-K$ degrees of
freedom. Having said that, we lose one more degree of freedom for every
entity in the panel, since for all i,
$\sum\limits_{t=1}^T \ddot{e}_{it} = 0$. Thus we end up with NT-K-N
degrees of freedom.

This info block is based on some information from Wooldridge (2012).
}

In R, fixed effects estimation is already implemented in the package
\texttt{plm}. Unfortunately Stata outputs its results in a somewhat
other way than \texttt{plm}. The main difference is that Stata reports
an intercept for fixed effects regressions. As the info block above
reveals, this is done by a a little extension of the basic theory of
fixed effects estimation. If you are interested in the details of how
Stata actually calculates the intercept, check out the next info block.

\infobox{Info: Fixed effects regression in Stata}{

According to Gould (2001) the fixed effects regression is implemented in
Stata in a slightly different way. An intercept $\beta_0$ will be added
into the base model, which amounts to
\[ y_{it} = \beta_0 + \beta_1 x_{it1} + ... +  \beta_K x_{itK}  + b_i + e_{it}.\]

Note that $b_i$ is time-constant, but differs from $a_i$ in the
previously described fixed effects regression in so far that it is
reduced by $\beta_0$.

If we would continue from here as in the previously described algorithm,
we would still end up without estimated intercept because $\beta_0$
would be cancelled out along with $b_i$. To avoid this, Stata calculates
not only the averages over time, but also calculates averages over both
entities and time from the model. We end up with

\[\bar{\bar{y}} = \beta_0 + \beta_1 \bar{\bar{x}}_{1} + ... + \beta_K \bar{\bar{x}}_{K}+ \bar{b} + \bar{\bar{e}},\]

where $\bar{\bar{y}}$,$\bar{\bar{x}}_k$,$\bar{b}$, and $\bar{\bar{e}}$
are the grand averages, defined as:

\[\bar{\bar{y}} =\frac{\sum_{i=i_1}^{i_N} \sum_{t=1}^T y_{it}}{N \cdot T}.\]

Where we have only subtracted the values averaged over time from the
base model, we now also add this last equation to it. This results in

\[y_{it} - \bar{y}_i + \bar{\bar{y}} = \beta_0 +  \beta_1(x_{it1}-\bar{x}_{i1} + \bar{\bar{x}}_{1}) + \beta_K(x_{itK}-\bar{x}_{iK} + \bar{\bar{x}}_{K}) + \bar{b}  +  (e_{it}- \bar{e}_i + \bar{\bar{e}}).\]

Now we postulate that $\bar{b} = 0$, and based on that we can carry out
a pooled OLS, which providing us with the estimates for
$\beta_k, k = 0,...,K$.

When we calculate the fixed effects from the fixed effects regression,
Stata returns the $b_i$. R would return the $a_i$ which differ from
$b_i$ by $\beta_0$. Therefore we can easily calculate the $a_i$ from the
results of Stata.

Looking at it the other way round, it is a little bit more complicated
to calculate the $b_i$ with R, because R does not return the overall
intercept. However, in the estimation of Stata, the postulate
$\bar{b} = 0$ directly influences the intercept $\beta_0$. So far, we
know $\bar{b} = 0$ and $\beta_0 + b_i = a_i \quad \forall i$.

By calculating the average over both sides of the latter equation, we
get
\[\frac{\sum_{i=i_1}^{i_N} \beta_0 + b_i}{N} = \frac{\sum_{i=i_1}^{i_N} a_i}{N}\]
which can be transformed to
\[\beta_0+\frac{\sum_{i=i_1}^{i_N}  b_i}{N} = \beta_0 = \frac{\sum_{i=i_1}^{i_N}  a_i}{N}=\bar{a}\]
or just \[\beta_0 = \bar{a}.\]
}

To exactly duplicate the exact results of Schularick and Taylor (2012a), we do
need this intercept. Since \texttt{plm} does not calculate it, we can
tweak the output of R and calculate it manually. As described in more
detail in the preceding info block, this can be done by calculating the
average over the fixed effects. However, to do so we first have to
estimate all individual fixed effects $\alpha_i$. Therefore, the
efficiency advantage from the fixed effects regression pretty much
disappears. As a consequence, in the following we will stick to the
dummy regression procedures we have already learned in the problem set
so far and will save you from having to understand the additional steps
required for \texttt{plm} to work.

This is okay and actually quite reasonable since Schularick and Taylor
use the fixed effects regression in the hope of improving estimates of
the mean over all countries at every respective point in time in the
data set (see exercise 1.2 c) for a more detailed assessment
of this approach). They speak of a `fixed country-and-year effects
regression', because both country and year are not continuous but
instead categorical variables and can both be considered as fixed
effects, as $\alpha_i$ is fixed over time and $\beta_t$ is fixed over
the entities. Moreover, in the proposed model we do not have a
continuous explanatory variable at all.

\subsubsection{\sf Exercise 1.2 b) -- The Relationship of Loans and Money}\label{exercise-1ux5f2-b-more-evidence-on-two-eras}

Before we loaded a prepared data set. In the following, we want to work
on the original data set that Schularick and Taylor provided. We can
load it with the function \emph{read.dta()} from the package
\texttt{foreign}, which can deal with data in native Stata format. We
find the data set in the file \texttt{panel17m.dta} and assign it to the
object \textbf{data}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package 'foreign' and data set}
\KeywordTok{library}\NormalTok{(foreign)}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.dta}\NormalTok{(}\StringTok{"panel17m.dta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The only continuous variable in the regression will be the dependent
variable -- this is the variable in whose global averages we are
interested. Before we begin with the regression we will copy the data
and will then work on that copy. Also, we will quickly check the classes
of the fixed effects variables. It is always useful to check for the
classes of a variable quickly, because sometimes one has to convert them
into other classes for a function to work on it. In this case, we will
have to make sure the categorical time and entity variables are treated
as categorical, which means they need to belong to the class
\texttt{factor}. As the check for the class reveals, they are not saved
in the class \texttt{factor}, we will plug them into \emph{as.factor()}
before regressing them.

With the first regression we want to calculate the global averages of
the logarithmized proportion of total loans to total money. That means,
we need to calculate this new variable first before we can carry out the
regression.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr) }\CommentTok{#load library for data manipulation again}
\NormalTok{data =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{logLoansbyMoney =} \KeywordTok{log}\NormalTok{(loans1/money)) }\CommentTok{#prepare data}
\end{Highlighting}
\end{Shaded}

If you want to verify the new series quickly, check out the next info
block.

\begin{infobox2}
\textbf{Info: Double-check of the new variable}

Let us verify the new series quickly by selecting the first four
observations for each country. This is a bit more complicated. In the
following code chunk we use the function \emph{transmute()}. This
function provides an intermediate approach between \emph{select()} and
\emph{mutate()}. Like \emph{select()}, it displays only specified
variables but can also transform variables into others like the function
\emph{mutate()} does. Thus the difference to \emph{mutate()} is that the
remaining unspecified variables from the data set are being dropped.
Before you proceed to check the prepared code chunk, read through the
comments.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select variables of interest}
\NormalTok{sel =}\StringTok{ }\KeywordTok{transmute}\DataTypeTok{(data, iso, year, loans1, money, }
		\DataTypeTok{LoansbyMoney =} \NormalTok{loans1/money, }\DataTypeTok{logLoansbyMoney)}

\CommentTok{# use head() function on each group instead of on whole data set}
\NormalTok{tmp1 =}\StringTok{ }\KeywordTok{do}\NormalTok{(}\KeywordTok{group_by}\NormalTok{(sel,iso),  }\KeywordTok{head}\NormalTok{(., }\DecValTok{4}\NormalTok{))}

\CommentTok{# use wrapper function 'head_g()' to call head() on grouped data frame}
\CommentTok{# The head_g() function's purpose is to reduce typing. }
\CommentTok{# It converts the grouped tbl data frame into a normal ungrouped one}
\CommentTok{# and applies head() on the ungrouped data frame. }
\KeywordTok{head_g}\NormalTok{(tmp1, }\DecValTok{56}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##    iso year   loans1     money LoansbyMoney logLoansbyMoney
## 1  AUS 1870       NA  54.30000           NA              NA
## 2  AUS 1871       NA  59.50000           NA              NA
## 3  AUS 1872       NA  68.50000           NA              NA
## 4  AUS 1873  64.0200  73.70000    0.8686567      -0.1408073
## 5  CAN 1870   0.0757        NA           NA              NA

   ¦    ¦    ¦        ¦         ¦            ¦               ¦
   
## 52 SWE 1873 223.6130 365.97079    0.6110133      -0.4926366
## 53 USA 1870       NA        NA           NA              NA
## 54 USA 1871       NA        NA           NA              NA
## 55 USA 1872       NA        NA           NA              NA
## 56 USA 1873       NA        NA           NA              NA
\end{verbatim}
}
%\end{verbatim}
%\vskip -0.8em \IN{15}{\Large$\bm{\vdots}$} 
%\vskip -2.2em \quad
%\begin{verbatim}

Now repeat these steps, but display the last two observations (i.e.~the
most recent two values) instead. The data set contains the data in
chronological order, thus the most recent values are saved in the last
few rows. Use the same data selection \textbf{sel} as before. Save the
result in \textbf{tmp2} and display it with \emph{head\_g()}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select variables of interest}
\NormalTok{sel =}\StringTok{ }\KeywordTok{transmute}\DataTypeTok{(data, iso, year, loans1, money, }
		\DataTypeTok{LoansbyMoney =} \NormalTok{loans1/money, }\DataTypeTok{logLoansbyMoney)}
\NormalTok{tmp2 =}\StringTok{ }\KeywordTok{do}\NormalTok{(}\KeywordTok{group_by}\NormalTok{(sel2, iso),  }\KeywordTok{tail}\NormalTok{(., }\DecValTok{2}\NormalTok{))}
\KeywordTok{head_g}\NormalTok{(tmp2, }\DecValTok{28}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##    iso year       loans1        money LoansbyMoney  logLoansbyMoney
## 1  AUS 2007 1.146298e+06 9.371746e+05    1.2231424       0.20142327
## 2  AUS 2008 1.300649e+06 1.081453e+06    1.2026866       0.18455786
   
   ¦     ¦   ¦            ¦            ¦            ¦                ¦
   
## 27 USA 2007 6.799500e+03 7.442500e+03    0.9136043      -0.09035773
## 28 USA 2008 7.194200e+03 8.140600e+03    0.8837432      -0.12358873
\end{verbatim}
}

We see that there are a lot of data missing for the early points in
time. Younger data are obviously more complete.
\end{infobox2}

As mentioned above, we will carry out the fixed effect regression as a
dummy regression with a categorical variable. According to the model's
definition above and also to the standard fixed effects regression
theory, we are not interested in an intercept from the OLS model (which
might sound confusing, since we just stated that we will have to compute
one manually to duplicate the results Stata generates. However, for us,
the overall intercept is only important in the second step -- in the
Stata-like prediction of the dependent variable in order to construct
the global averages). Hence, we will add \texttt{-1} to the formula in
the regression model.

In the following code chunk, the code has been prepared for you. Read
through the comments and try to understand each line, as you will have
to the coding yourself soon.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check the classes of 'iso' and 'year'. }
\KeywordTok{sapply}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, iso, year), class) }
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##         iso        year 
## "character"   "integer"
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The 'apply()' function literally applies the function 'class()', }
\CommentTok{# called as second argument of 'apply()', on all columns }

\CommentTok{# since 'iso' and 'year' are not class 'factor' , we convert them into it}
\NormalTok{fe.reg1 =}\StringTok{ }\KeywordTok{lm}\NormalTok{(logLoansbyMoney ~}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(iso) +}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(year) -}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =} \NormalTok{data)}
\CommentTok{# note: lm() knows to handle class 'character' as factor but not class 'numeric'.}
\CommentTok{# yet to make sure both variables are treated as factors, we convert them both}
\end{Highlighting}
\end{Shaded}

Now we can use the estimated model to determine predicted values of the
dependent variable \textbf{LogLoansbyMoney}. Since we want global
values, it is now time to calculate the overall intercept from the fixed
country effects. This will be done implicitly for you in the function
\emph{predict.fe()}. This function is not part of a package but is
written specifically to duplicate results from fixed effects regression
in Stata, which provides the mean over the time constant fixed effects
as overall intercept. \emph{predict.fe()} expects three arguments: the
estimated linear model, a vector containing the predictors, and the name
of a categorical variable which serves as index variable. We will choose
the fitted values as predictors, thus we can just extract them from the
data frame and save them in \textbf{newdata}. We then calculate the
predicted values and finally we are going to view the first few values.
These three steps have been done for you, you can just run the code
chunk to see the results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata =}\StringTok{ }\KeywordTok{select}\NormalTok{(data, iso, year) }\CommentTok{# choose the predictors}

\CommentTok{# get predicted values from estimated model}
\NormalTok{data$logLoansbyMoney_mean1 =}\StringTok{ }\KeywordTok{predict.fe}\NormalTok{(}\DataTypeTok{object =} \NormalTok{fe.reg1,}\\
					\DataTypeTok{newdata =} \NormalTok{newdata,}\DataTypeTok{ index =} \StringTok{"iso"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## dummies-1.5.6 provided by Decision Patterns
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show first few predicted values}
\CommentTok{# we don't print all - it would print 1946 values, }
\CommentTok{# since that is the column length}
\CommentTok{# instead you can check it out in the data explorer}
\KeywordTok{head}\NormalTok{(data$logLoansbyMoney_mean1)  }
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] -0.6496375 -0.4041150 -0.3837891 -0.2700776 -0.1942220 -0.1606306
\end{verbatim}
}


Let us have a look at the results. We use the function \emph{ggplot()}
to plot the data, so we need to load the package \texttt{ggplot2}. In
what follows, we will also save the layers of the plots separately in
order to facilitate their later reuse. We still need to initialize a
ggplot skeleton, which we have set up to contain the years as x-values
already. We assign this ggplot-skeleton to \textbf{base\_plot}. To
create the actual plots, we then have to add layers to the skeleton.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# base plot}
\NormalTok{base_plot =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{data,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{year))}

\NormalTok{## layer: log loans by money}
\NormalTok{l_llbm_m1 =}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \NormalTok{logLoansbyMoney_mean1), }\DataTypeTok{size =} \DecValTok{3}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"black"}\NormalTok{)}
\NormalTok{base_plot +}\StringTok{ }\NormalTok{l_llbm_m1}
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/1_2_b___14-1.pdf}

The plot looks quite familiar. We have seen a similar one before when we
plotted the equal weighted averages of total assets by money over time.
Again, in the first half the relationship remains fairly stable, whereas
then the characteristic of the graph changes, indicating that total
loans (or total credit) gains weight relatively to money.

\subsubsection{\sf Exercise 1.2 c) -- The Relationship of Assets and Money}\label{exercise-1ux5f2-c-global-averages-over-time-of-total-assets-by-money-with-a-fixed-effects-regression}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package 'foreign' and data set}
\KeywordTok{library}\NormalTok{(foreign)}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.dta}\NormalTok{(}\StringTok{"panel17m.dta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

First of all, we need to create the new logarithmic series of total
assets by money again. As before, this has been done for you. To check
this creation of the variable, we will display the first 20 values with
the function \emph{head()}. To have a quick glance you can uncomment the
last line of code in the next code chunk and vary the parameter $n$ to
show as many results as you want. Of course you can also have a look
into the data explorer.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create new series}
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{data = }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{logAssetsbyMoney =} \KeywordTok{log}\NormalTok{(bassets2/money))}
\CommentTok{# change function tail() to head() if you want to see early data}
\CommentTok{# change parameter 'n' to a different value to show more years}
\CommentTok{# as.data.frame(do(select(group_by(data, iso), year, logAssetsbyMoney), tail(., n = 2)))}
\end{Highlighting}
\end{Shaded}

We saw in part a) that the variable \textbf{logAssetsbyMoney} has no
value for any country in the first year 1870. In the succeeding years
there are still many missing data points, but the data quality actually
gets better quickly.

\begin{infobox2}
\textbf{Info: Check whether data quality of 'logAssetsbyMoney' improves after the first few years}

Let us verify whether the data quality actually gets better after the first years. If
you check the next code chunk, you will get to see how many values are
missing in each year.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# group data by year }
\NormalTok{data_byY =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(data, nyear)}
\CommentTok{# then sum up number of missing values of 'logAssetsbyMoney' per group}
\KeywordTok{missing = summarise}\NormalTok{(data_byY, }\DataTypeTok{missing_values =} \KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(logAssetsbyMoney)))}
\CommentTok{# show first n values}
\CommentTok{# (recall head_g() is a wrapper for head() on grouped data)}
\KeywordTok{head_g}\NormalTok{(}\DataTypeTok{x =} \NormalTok{missing, }\DataTypeTok{n =} \DecValTok{35}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{SAMEPAGE}
{\small
\begin{verbatim}
##    year missing_values
## 1  1870             14
## 2  1871             12
## 3  1872             12
## 4  1873             11

   ¦     ¦              ¦   

## 34 1903              2
## 35 1904              2
\end{verbatim}
}
\end{SAMEPAGE}

\vskip -3em The first year lacks all observations for all 14 countries. In the next
two years, there are observations for two countries (14 countries minus
12 missing values) and after ten years, observations are available for
more than half of the countries.
\end{infobox2}

As a consequence, values for year 1870 cannot be used as the base values
for a dummy regression (and thus we do not get a coefficient for this
year straight away, or in other words, its coefficient is zero,
therefore R will omit it). For the conversion of the categories into
dummy variables, we will have to use another year instead. By default,
the next year is used as base year, provided not all values are missing
as well.

Now, let us carry out nearly the same regression as above, except that
we change the dependent variable to \textbf{logAssetsbyMoney}. This
means we will be regressing \textbf{logAssetsbyMoney} on \textbf{iso}
and \textbf{year}, both independent variables being treated as
categorical variables again. Assign the results from the regression to
\textbf{fe.reg.2}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# second regression}
\NormalTok{fe.reg2 =}\StringTok{ }\KeywordTok{lm}\NormalTok{(logAssetsbyMoney ~ }\KeywordTok{as.factor}\NormalTok{(iso) + }\KeywordTok{as.factor}\NormalTok{(year) - }\DecValTok{1}\NormalTok{, }\DataTypeTok{data = }\NormalTok{data)}
\end{Highlighting}
\end{Shaded}

Remember from the previous part of the exercise that next we need to
calculate the predicted values of the dependent variable, which in this
case are the global averages of the logarithmized fraction of assets by
money. Like before, we can compute them with \emph{predict.fe()} and we
will save the values in a variable \textbf{logAssetsbyMoney\_mean1} in
the data frame \textbf{data}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata =}\StringTok{ }\KeywordTok{select}\NormalTok{(data, iso, year) }\CommentTok{# choose the predictors}
\NormalTok{data$logAssetsbyMoney_mean1 =}\StringTok{ }\KeywordTok{predict.fe}\NormalTok{(fe.reg2, newdata, }\StringTok{"iso"}\NormalTok{)}
\CommentTok{# }

\CommentTok{# Once you have correctly calculated the global averages of assets by money in }
\CommentTok{# logs, you can check out the results.}
\CommentTok{# Probably you don't want to print all values on the screen, }
\CommentTok{# but check out the results for the early years}
\KeywordTok{head}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, year, logAssetsbyMoney_mean1))}
\end{Highlighting}
\end{Shaded}

\begin{SAMEPAGE}
{\small
\begin{verbatim}
##   year logAssetsbyMoney_mean1
## 1 1870              0.2982835
## 2 1871              0.2982835
## 3 1872              0.3507694
## 4 1873              0.3553765
## 5 1874              0.3893023
## 6 1875              0.4125875
\end{verbatim}
}
\end{SAMEPAGE}

As you saw before, there were no data for year 1870 in the original
logarithmic ratio of assets by money. Nevertheless, the last call in the
chunk reveals that we get a global average value for 1870, and you
perhaps have noticed that it is the same value as for 1871. This has its
reason again in the way Stata handles fixed effects regressions. Since
1871 serves as base year, its coefficient is zero. The dummy regression
method we use to achieve the same results as from a fixed effects
regression does not give us an intercept, and due to the missing data in
1870 there would be no coefficient for this year. This can be
interpreted as zero. Then it follows that the predicted value that was
reported is equal to the overall intercept that Stata and the function
\emph{predict.fe()} add when they compute the results from a fixed
effects regression. This value should be equal to the mean value of the
country fixed effects.

Let us verify this quickly: the country fixed effects themselves
correspond to the country dummy coefficients from the regression, which
are reported by R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We set up the model in a way where the year dummies are reported as the first}
\CommentTok{# 14 coefficients}
\NormalTok{coefs_sample =}\StringTok{ }\KeywordTok{head}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fe.reg}\FloatTok{.2}\NormalTok{),}\DecValTok{ 18}\NormalTok{) }
\NormalTok{coefs_sample}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##   as.factor(iso)AUS   as.factor(iso)CAN   as.factor(iso)CHE 
##          0.11681981          0.40255202          0.75972598 
##   as.factor(iso)DEU   as.factor(iso)DNK   as.factor(iso)ESP 
##          0.90051023          0.43701591         -0.45243752 
##   as.factor(iso)FRA   as.factor(iso)GBR   as.factor(iso)ITA 
##         -0.21529438          0.25086552          0.07427363 
##   as.factor(iso)JPN   as.factor(iso)NLD   as.factor(iso)NOR 
##          0.42503104          0.43212185          0.71991497 
##   as.factor(iso)SWE   as.factor(iso)USA as.factor(year)1872 
##          0.02940666          0.29546379          0.05248587 
## as.factor(year)1873 as.factor(year)1874 as.factor(year)1875 
##          0.05709298          0.09101873          0.11430392 
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# now we calculate their mean value}
\KeywordTok{mean}\NormalTok{(coefs_sample[}\DecValTok{1}\NormalTok{:}\DecValTok{14}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 0.2982835
\end{verbatim}
}

Now let us plot these new global averages like we did before with the
first set of global averages that we calculated. We need to create a
layer containing the y-values that we can add to our base ggplot
skeleton, which contains the years as x-values.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# base plot}
\NormalTok{base_plot =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{data,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x = }\NormalTok{year))}
\NormalTok{l_labm_m1 =}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \NormalTok{data, }
                                 \KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \NormalTok{logAssetsbyMoney_mean1), }
                                 \DataTypeTok{size =} \DecValTok{3}\NormalTok{,}
                                 \DataTypeTok{color = }\StringTok{"grey45"}\NormalTok{)}
\NormalTok{base_plot +}\StringTok{ }\NormalTok{l_labm_m1 }
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/1_2_c___21-1.pdf}

Although the plot in exercise 1, where we plotted the global averages
directly from calculating equal weighted averages, is not identical, the
difference is rather marginal and for the detection of two eras in the
data, the easier way would probably have been sufficient.

From a statistical point of view, Schularick and Taylor's fixed effects
regression to calculate the mean values can indeed make sense. The fixed
effects regression interpolates missing values based on the data of each
country. However, improvements which were made in the paper by using
fixed effects regressions for the calculation of the means are rather
neglectable. As we have seen, the characteristics proving the existence
of two eras are eventually the same.

The sense of using a fixed effects regression approach becomes even more
questionable if you consider, that it is based on a linear regression
approach. \emph{Schularick and Taylor} try to show, that the underlying
values in their data set do not show a consistently linear relationship
but rather have a knee at which point the characteristics of the curve
change, yet they use this approach which is actually based on a
consistently linear relationship. It probably comes down to the question
whether it deteriorates the results. This does not seem to be the case,
their results still show the structural break. This was what they wanted
to show and although the fixed effects approach to calculate the mean
values would rather even out the knee in the data, we saw that the plot
still shows the same characteristics as the weighted average plot.
Therefore we can conclude, that it is okay in this scenario.

\subsubsection{\sf Exercise 1.3 -- Two Eras of Financial Development: Combined Findings}\label{exercise-1ux5f3-global-averages-over-time-findings-combined}

This part of the exercise combines the results from the previous parts
and evaluates them in further detail. Just load the file
\texttt{ex1\_1full.RData} which contains all the necessary objects for
this exercise.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\DataTypeTok{file =} \StringTok{"data_1_3.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection*{a) \sf \textbf{The underlying data}}\label{a-the-underlying-data}

So far we have seen two ways of how to create global averages for some
aggregated data from the data set provided by Schularick and Taylor. The
preceding parts have explained in detail the creation of plots showing
the global changes in the relationship of two credit aggregates with
broad money. We have set up these plots with functions from the package
\texttt{ggplot2} and in this process we created layers enabling us to
combine these plots easily, as we will see very soon. In addition to
this, we are going to create one more layer showing the logarithmic
quotients of total bank assets by broad money per country which we used
before to create the global averages. Just check the following code
chunk to create a plot of these values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package 'ggplot2'}
\KeywordTok{library}\NormalTok{(ggplot2)}

\CommentTok{# create layer and plot it by adding it to the ggplot skeleton 'base plot'}
\NormalTok{l_labm_all =}\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{year, }\DataTypeTok{y =} \NormalTok{logAssetsbyMoney, }\DataTypeTok{color =} \NormalTok{iso), }\DataTypeTok{data =} \NormalTok{data) }
\NormalTok{base_plot +}\StringTok{ }\NormalTok{l_labm_all  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
    \centering
\includegraphics[width=24cm,height=10.5cm]{creditboomsgonebust_output_solution_files/figure-latex/1_3__23-1.pdf}
\end{figure}

Next you can add the layer containing the average values of total assets
by money to the plot. The layer is saved in the object
\textbf{l\_labm\_m1}. Simply add it to the two layers of the preceding
code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base_plot +}\StringTok{ }\NormalTok{l_labm_all +}\StringTok{ }\NormalTok{l_labm_m1}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
    \centering
\includegraphics[width=24cm,height=10.5cm]{creditboomsgonebust_output_solution_files/figure-latex/1_3__24-1.pdf}
\end{figure}

\begin{spacing}{1.08}
If you did the preceding parts of the exercise, you have already seen
the graph of the global averages and its structural break during the
time of World War 2. Now you can view it in combination with its
underlying data and observe that for the years around the Second World
War the quality of the underlying data is a bit weak, similarly to the
early years in the data set (we saw that already). This of course does
not change the deduction that the relation of total assets and money
changed after World War 2 in comparison to before.

Schularick and Taylor claim that the global averages reflect a collapse
of money and credit aggregates due to the Great Depression, but this
statement cannot be completely confirmed. For a start, the ratio between
money and credit would remain stable if both aggregates collapsed
(assuming they collapse at the same level -- anyhow, the plot does not
show this). Moreover, the visible, rather weak underlying data quality
during this time frame raises some doubts as to whether the lower levels
of the assets to money ratio was not mainly induced by the War.

Concentrating once more on the underlying data we see that the spread
between the curves in the first era is slightly higher. However, apart
from the lines for Spain and Switzerland which grow noticeably, data
from most other countries remain at the same level. This means that in
these latter countries the relationship of total assets and money did
not change too much during the first era. This stable relationship is
also shown by the global average which remains slightly above zero
during the first era.

Since it is hard to follow the lines for each country we do not present
the data for the ratio of total loans to money in the same way. Instead,
we create a separate plot for every country and merge these plots into
one figure. Also, let us add the data from the preceding plot.

For a complete understanding of the next code chunk two remarks may be
helpful. First, the tidiest way to plot more than one variable at the
same time with \emph{ggplot()} is to use data in long format. This
means, that all your values for all variables you want to plot show up
in one column, and in another column it is noted to which variable the
respective value belongs to. When you melt data, you bring it from wide
format where each variable is saved in its own column (as with our data)
into long format. Second, you will probably notice in the code the
symbols \texttt{\%\textgreater{}\%}. You can read them as \texttt{then}.
For more info, check out the info block.
\end{spacing}

\infobox{Info: Chaining (\%\textgreater{}\%)}{
\begin{spacing}{1.1}
\vskip -1em Some packages, like \texttt{dplyr} and \texttt{reshape2} let you use a
concept called chaining. If you do not want to save intermediate results
in your workspace but are also trying to avoid difficult convoluted
commands, this concept can be very helpful. Usually, you would end up
reading your code from the inside to the outside. With chaining, which
is done by using the `then'-operator \texttt{\%\textgreater{}\%}, you
can avoid that. Basically, it lets you write your code in an algorithm
like pattern. Compare the code in the following task block to this
algorithm which reads as follows:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  1.) Select certain variables to plot, THEN
\item
  2.) Melt these variables according to the arguments supplied to
  \emph{melt()}, THEN
\item
  3.) Plot the melted data
\end{itemize}
\end{spacing}
}

\vskip 2em \begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2)}
  \CommentTok{# select variables to plot}
  \KeywordTok{select}\NormalTok{(data, iso, year, logAssetsbyMoney, logLoansbyMoney) %>%}\StringTok{ }
\StringTok{  }
\StringTok{  }\CommentTok{# melt variables, keep variables 'iso' and 'year' as indices (in addition to }
\StringTok{  }\CommentTok{# the variable names)}
\StringTok{  }\KeywordTok{melt}\NormalTok{(}\DataTypeTok{id.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{, }\StringTok{"year"}\NormalTok{)) %>%}\StringTok{ }

\StringTok{  }\CommentTok{# set up the plot:}
\StringTok{  }\CommentTok{# the dot passes the data set which is selected in the preceding}
\StringTok{  }\CommentTok{# steps to argument 'data'}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data = }\NormalTok{., }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{year, }\DataTypeTok{y =} \NormalTok{value, }\DataTypeTok{colour =} \NormalTok{variable)) +}\StringTok{ }
\StringTok{  }\CommentTok{# make it a scatter plot}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }
\StringTok{  }\CommentTok{# remove label of y-axis}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) +}\StringTok{ }
\StringTok{  }\CommentTok{# show each country in separate plot}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(~iso, }\DataTypeTok{ncol =} \DecValTok{4}\NormalTok{) +}\StringTok{ }
\StringTok{  }\CommentTok{# change labels in legend }
\StringTok{  }\KeywordTok{scale_color_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Series (in logs)"}\NormalTok{, }
                         \DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\StringTok{"logAssetsbyMoney"}\NormalTok{, }\StringTok{"logLoansbyMoney"}\NormalTok{),}
                         \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"log(assets/money)"}\NormalTok{, }\StringTok{"log(loans/money)"}\NormalTok{)) +}\StringTok{ }
\StringTok{  }\CommentTok{# move legend to bottom of plot}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/1_3__25-1.pdf}

In the plots, the relationship between both ratios and thus eventually
the relationship between total bank loans and total bank assets seems to
vary to some extent for the different countries. In some countries, for
instance Germany and Italy, the variation in the relationship between
the credit aggregates remains low but their relationship towards money
changes constantly. In others, like Spain and Norway, the relationship
between the credit aggregates is not as stable, but at least for Norway
the relationship of the credit aggregates towards broad money remains
roughly stable in the first era. This stable relationship in the first
era is even more visible for Great Britain and the USA.

Australia and Canada attract attention because in contrast to many other
countries their credit to money ratios do not only remain stable but
even decrease in the first era. They only recover in the second era but
do not really exceed the early values from the first era. Even for these
countries, however, the splitting up of the time into two eras is
obviously reasonable. During the second era a strong growth is notable
throughout most countries.

Looking at the plots we can also recognise easily the lack in data for
some series again. Both France and Japan lack data for assets by money
for the first era completely. For Denmark, the series interfere at that
era, so the plot could be misleading if you do not look carefully. If
you want to know how many data points for each series and country are
missing precisely, you can run the next code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{group_by}\NormalTok{(data,iso) %>%}\StringTok{ }
\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{missing_AbyM =} \KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(logAssetsbyMoney)), }
          \DataTypeTok{missing_LbyM =} \KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(logLoansbyMoney)))}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [14 x 3]
## 
##    iso missing_AbyM missing_LbyM
## 1  AUS            5           10
## 2  CAN           10           10
## 3  CHE           17           36
## 4  DEU           36           36
## 5  DNK           20           20
## 6  ESP           36           36
## 7  FRA           79           40
## 8  GBR           21           17
## 9  ITA           19           19
## 10 JPN           85           45
## 11 NLD           36           33
## 12 NOR           35            0
## 13 SWE            1            1
## 14 USA           21           33
\end{verbatim}
}

The numbers validate that for these two series the data quality of
France and Japan are not on par with the other countries and also
reveals which countries have more missing values than others in the data
set.

\subsubsection*{b) \sf \textbf{Global averages over time in
comparison}}\label{b-global-averages-over-time-in-comparison}

Let us combine the plots of the global averages that we have produced in
the preceding parts of exercise one. This is simple -- due to the
flexibility of the package \texttt{ggplot2} we can easily merge the
layers that were created in the previous part of the exercise
(\textbf{l\_labm\_m1} and \textbf{l\_llbm\_m1}) by adding them both to
the ggplot-skeleton \textbf{base\_plot}. Go ahead and do this and also,
in order to suppress the axis label from the print, add `+ ylab(``'')'
to your command, otherwise R will use the name of just one curve as
label for the y-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base_plot +}\StringTok{ }\NormalTok{l_labm_m1 +}\StringTok{ }\NormalTok{l_llbm_m1 +}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/1_3__27-1.pdf}

Now you see that the curves show indeed a very similar behaviour. The
logarithmic total loans by money ratios, represented by the black dots,
are a bit more volatile in the beginning than the total assets by money.
But due to the poor data quality of the first few years we should not
put too much emphasis on this part of the data. Apart from those early
years both lines maintain a strong similarity. Both curves clearly show
the beginning of what Schularick and Taylor call a new era after World
War~2.

They also state that the decoupling of the credit aggregates reflect the growth in leverage funding and augmented funding through non-monetary liabilities. These sources of funding have increased even more in the last decade. To prevent crises in times where markets are in financial distress, central banks might have to put more and more effort into rescuing banks, eventually having to underwrite the whole funding market. In other words, stability in the funding markets becomes more important than ever and the link between money and credit gets more lose.



\eject

\subsection{\sf Exercise 2 -- The Aftermath of Financial
Crises}\label{exercise-2-the-aftermath-of-financial-crises}

The topic of this exercise is the evaluation of the aftermath of crises
on money and credit aggregates, real economic effects, and price
development.

Crises are for our purposes defined as ``events during which a country's
banking sector experiences bank runs, sharp increases in default rates
accompanied by large losses of capital that result in public
intervention, bankruptcy, or forced merger of financial institutions''
(Schularick and Taylor, 2012a, p.~1038). Within the provided data set
can be found a dummy variable \textbf{crisisST}, which indicates
whenever a crisis took place in a country.

In the previous exercises, the prevalence of two eras with different
characteristics with respect to money, credit, and real economic
aggregates was demonstrated for the time before and after World War 2.
Thus in this exercise it will not only be examined whether there were
measurable effects emanating from crises on the above listed economic
indicators but also how these effects changed between the two eras.

This exercise is divided into three parts. In each of these parts 
we will analyse the reaction of specific financial and economic indicators to a crisis.
In particular, we evaluate, in comparison to normal times, how the indices react 
within the next five years after a crisis.

In the first part this will be done for the two credit aggregates which we saw in the 
preceding exercise. Again we will compare them with the broad money aggregate and it will 
be interesting to see to which extent the change in the respective relationship is 
reflected here. In addition to this the given crises data will be evaluated in brief in the first part. 

In part two the focus will additionally be a technical point: the preparation of the data. For both narrow and broad money growth as
well as inflation rates we are going to learn how to prepare the data from the scratch before we 
evaluate them.

In the last part you have to apply the knowledge aquired from the second part. Based on this, the effects
of financial crises on the real economy are investigated, which are actually amplified in the recent era.

\subsubsection{\sf Exercise 2.1 -- Crisis Effects on Credit Development
}\label{exercise-2ux5f1-the-impacts-on-the-first-five-years-after-a-crisis}

We start this exercise as we started the preceding ones, with loading
the data set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package foreign and data set}
\KeywordTok{library}\NormalTok{(foreign)}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.dta}\NormalTok{(}\StringTok{"panel17m.dta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection*{a) \sf \textbf{Some infos about the
crises}}\label{a-some-infos-about-the-crises}

Since we will be analysing the effects of crises on the subsequent years
in the following tasks, let us begin with evaluating when and where the
crisis actually took place. A crisis is indicated in the provided data
set by a `1' in the variable \textbf{crisisST}, `0' indicating that no
crisis took place. Let us modify the data so that crises are easily
observable.

Since we are only interested in crisis years, we start with extracting
these years with the function \emph{filter()} provided by the package
\texttt{dplyr}. Then we use the function \emph{split()} on those years
to list the crisis years by country. Other than with most functions in
this problem set, we cannot pass a data frame to \emph{split()}, instead
we have to specify the explicit variables, which can be done easily by
using the \texttt{\$} sign.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package dplyr and reshape}
\KeywordTok{library}\NormalTok{(dplyr) }

\CommentTok{# only select rows of variables year, iso and crisisST with crisisST = 1}
\NormalTok{data_f1 =}\StringTok{ }\KeywordTok{filter}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, year, iso, crisisST), crisisST ==}\StringTok{ }\DecValTok{1}\NormalTok{)}

\CommentTok{# split year by iso (as often before, iso serves as category)}
\KeywordTok{split}\NormalTok{(data_f1$year, }\KeywordTok{as.factor}\NormalTok{(data_f1$iso))}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## $AUS: 1893 1989
## $CAN: 1873 1907 1923
## $CHE: 1870 1910 1931 2008
## $DEU: 1873 1891 1901 1907 1931 2008
## $DNK: 1877 1885 1902 1907 1921 1931 1987
## $ESP: 1883 1890 1913 1920 1924 1931 1978 2008
## $FRA: 1882 1889 1907 1930 2008
## $GBR: 1873 1890 1974 1984 1991 2007
## $ITA: 1873 1887 1891 1907 1921 1930 1935 1990 2008
## $JPN: 1882 1900 1904 1907 1913 1927 1992
## $NLD: 1893 1907 1921 1939 2008
## $NOR: 1899 1922 1931 1988
## $SWE: 1878 1907 1922 1931 1991 2008
## $USA: 1873 1884 1893 1907 1929 1984 2007
\end{verbatim}
}

According to this output, the number of crises varies from country to
country, some countries facing as many as nine crises but others facing
``only'' two. Interesting is also the chronology of crises, which we
will demonstrate in a chart over time. Fill in the missing values and
then check the chunk. If you need help, click the \texttt{hint} button

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# Uncomment the following lines and assign the correct variable names}
\CommentTok{# ggplot(data, aes(x = ?, y = ?)) + }
\CommentTok{#  geom_bar(aes(fill = ?), stat = "identity", position = "stack") +  }
\CommentTok{#  scale_y_continuous(minor_break = FALSE, breaks = seq(0, 9))}
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{year, }\DataTypeTok{y =} \NormalTok{crisisST)) +}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =} \NormalTok{iso), }\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{, }\DataTypeTok{position =} \StringTok{"stack"}\NormalTok{) +}\StringTok{  }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{minor_break =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/2_1__30-1.pdf}

It is notable that in the first era there seem to be three major crises
affecting many countries at the same time and a lot of small crises. The
major crises are the panic of 1873 which is, according to Krugman
(2010), the first crisis widely described as depression with the
following years being characterised by deflation and instability; the
great depression in 1931 -- here it should be noted that it is
definitely disputable why there is no sign of a financial crisis
indicated for Great Britain. Also, authors (e.g.~Hart, Jonker, \& van
Zanden (1997)) seem to agree on a late start of the great depression in
the Netherlands, not as late as 1939 though. The third major crisis is
the panic of 1907, which was caused by an unfortunate series of events,
where an earthquake in San Francisco led to the stock market coming down
significantly and a subsequent tightening of money. Of course, the
fourth and youngest major crisis is the financial crisis from 2007/2008.
It seems to be the only crisis affecting many countries at the same time
in the post World War 2 era.

On the whole it seems like financial crises have occurred less often in
the post World War 2 era according to these data, whether they affected
only one or two countries or more.

\subsubsection*{b) \sf \textbf{The aftermath of a crisis on total loans, total assets
and
money}}\label{b-the-aftermath-of-a-crisis-on-total-loans-total-assets-and-money}

In the data set provided by Schularick and Taylor, there are
observations within the variables \textbf{loans1}, \textbf{bassets2},
and \textbf{money} which differ in currency and also in units (for
example, total US loans might be given in billion dollars and total
loans for Spain in million pesetas). Therefore we cannot compare the
values themselves but their growth rates.

Consequently, to compare them we have to create new variables containing
the growth rates first and can then use them to compute some aggregates.
These aggregates will then be used to plot some bar charts.

In the data file that we are going to read in, you will find the data
for three variables, namely \textbf{D log(Bank Assets)}, \textbf{D
log(Bank loans)}, and \textbf{D log(Broad Money)}. These data have
already been prepared so that you are able to plot them straight away.
Let us start with reading in the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in the data, then show a few columns}
\KeywordTok{load}\NormalTok{(}\StringTok{"data_2_1.RData"}\NormalTok{)}
\NormalTok{tmp.prep.fig4}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [42 x 4]
## Groups: era, afterCrisis
## 
##        era afterCrisis           variable      means
## 1  PostWW2      Normal D log(Bank Assets) 0.10761365
## 2  PostWW2      Normal  D log(Bank Loans) 0.11130646
## 3  PostWW2      Normal D log(Broad Money) 0.08629811
## 4  PostWW2           0 D log(Bank Assets) 0.09534134
## 5  PostWW2           0  D log(Bank Loans) 0.08601373
## 6  PostWW2           0 D log(Broad Money) 0.08594301
## 7  PostWW2           1 D log(Bank Assets) 0.06576527
## 8  PostWW2           1  D log(Bank Loans) 0.09289544
## 9  PostWW2           1 D log(Broad Money) 0.08886354
## 10 PostWW2           2 D log(Bank Assets) 0.11165752
## ..     ...         ...                ...        ...
\end{verbatim}
}

As you can see, the data is, with all values being in one column, already
in long format which lets us plot it easily. If you want to, you can
check it out completely by clicking the \texttt{data}-button. In column
`era' you find two categories `PostWW2' and `PreWW2', and column
`afterCrisis' shows whether a value is calculated within 5 years after a
crisis happened and if this is not the case, it is indicated by
`Normal'. In the last column you find the aggregated mean values and in
the column `variable' you find out to which variable this value refers
to.

We can now read the output from the left to the right side. For example
we see that during the Post World War 2 era in the defined `normal
times' with no crisis having happened recently, the average growth rate
of total bank assets was with nearly eleven percent slightly lower than
the average growth rate of total loans during the same period. To get an
even better impression of these numbers, we are going to present them
using a bar chart. This will make it easier to grasp the findings.

There are different ways to create a bar chart, and in this exercise you
are going to do it using the function \emph{barchart()} from the package
\texttt{lattice}. It is easy to create and it allows us to actually
display the data in two charts next to each other so that we will be
able to compare pre and after crisis results.

In the following code chunk you should pay attention to the comments and
particularly to the first two lines within the function
\emph{barchart()}. These are responsible for your basic plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# figure 4 - pre/post crisis bar chart}
\CommentTok{# load package for barchart()}
  \KeywordTok{library}\NormalTok{(lattice) }
\CommentTok{# load package to change colors for design}
  \KeywordTok{library}\NormalTok{(RColorBrewer) }

\CommentTok{# create bar chart}
\KeywordTok{barchart}\NormalTok{(means ~}\StringTok{ }\NormalTok{afterCrisis |}\StringTok{ }\NormalTok{era,   }
           \CommentTok{# y ~ x for each category in variable behind '|'}
           \CommentTok{# --> two categories in era: plot separately into two different panels}
           \DataTypeTok{groups =} \NormalTok{variable,         }\CommentTok{# one bar for each variable in 'variable'}
           \DataTypeTok{index.cond =} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)), }\CommentTok{# change order of panels}
           \DataTypeTok{data =} \NormalTok{tmp.prep.fig4,      }\CommentTok{# data selection}
           \DataTypeTok{main =} \StringTok{"Aggregates"}\NormalTok{,}
           \DataTypeTok{origin =} \DecValTok{0}\NormalTok{,              }\CommentTok{# changes appearance of bars}
           \DataTypeTok{auto.key =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{space =} \StringTok{"bottom"}\NormalTok{, }\DataTypeTok{columns = }\DecValTok{3}\NormalTok{), }\CommentTok{# change legend}
           \DataTypeTok{par.settings =} \KeywordTok{simpleTheme}\NormalTok{(}\DataTypeTok{col = }\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Blues"}\NormalTok{)) }\CommentTok{# change design}
           
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\vskip 0.7em \includegraphics{creditboomsgonebust_output_solution_files/figure-latex/2_1__32-1.pdf}

\vskip 1.4em For all three aggregates total assets, total loans and money we can see
very different dynamics for before and after the World War 2. In the
first era, growth of all the three indicators is greatly reduced during
a crisis year and it takes pretty much the full five years for them to
recover.

In contrast to that, the effect on all three variables seems less
noticeable in the time after World War 2. Here the total bank assets
seem to effected the most, but loans and money remain at a very high
level. According to Schularick and Taylor, a possible explanation for
this behaviour could be the influence of central banks, which took
actions to prevent money from collapsing and so managed to keep up bank
lending.

Finally, we can calculate the average total impact of a crisis on the
development of the respective economic indicators. Basically we will sum
up by how much in total the data series were affected in the first five
years after a crisis. That is, we are going to calculate the differences
of post crisis years to normal years and then sum up these differences.
Schularick and Taylor (2012a, p.~1040) call this the ``cumulative level
effects (relative to trend growth in non-crisis years five years after
the event)''.

From before, \textbf{tmp.prep.fig4} is grouped by \textbf{era} and
\textbf{afterCrisis}. If the latter group would remain, we couldn't sum
up over it. Thus we want to group by \textbf{era} and \textbf{variable}
instead, because that way we can sum over \textbf{afterCrisis} and keep
the variables and eras as index. Use the function group\_by() from the
package \texttt{dplyr} that you already got to know in preceding
exercises and assign your result to \textbf{tmp.cumlog}. If you need
help, click on \texttt{hint}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package 'dplyr'}
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{tmp.cumlog =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(tmp.prep.fig4, variable, era)}
\end{Highlighting}
\end{Shaded}

We can now create the differences between post crisis years and normal
years. Within each group, normal years are saved at position one, so we
can use a combination of the function \emph{mutate()} and within it the
function \emph{head()} to subtract the non-crisis year value from each
other value to see by how much in each post crisis year growth was
reduced in comparison to normal levels.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create differences between post crisis years to non crisis year}
\NormalTok{tmp.cumlogD =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(tmp.cumlog, }\DataTypeTok{Dmeans =} \NormalTok{means - }\KeywordTok{head}\NormalTok{(means, }\DecValTok{1}\NormalTok{))}
\NormalTok{tmp.cumlogD}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [42 x 5]
## Groups: variable, era
## 
##        era afterCrisis           variable      means        Dmeans
## 1  PostWW2      Normal D log(Bank Assets) 0.10761365  0.0000000000
## 2  PostWW2      Normal  D log(Bank Loans) 0.11130646  0.0000000000
## 3  PostWW2      Normal D log(Broad Money) 0.08629811  0.0000000000
## 4  PostWW2           0 D log(Bank Assets) 0.09534134 -0.0122723042
## 5  PostWW2           0  D log(Bank Loans) 0.08601373 -0.0252927336
## 6  PostWW2           0 D log(Broad Money) 0.08594301 -0.0003550985
## 7  PostWW2           1 D log(Bank Assets) 0.06576527 -0.0418483776
## 8  PostWW2           1  D log(Bank Loans) 0.09289544 -0.0184110287
## 9  PostWW2           1 D log(Broad Money) 0.08886354  0.0025654304
## 10 PostWW2           2 D log(Bank Assets) 0.11165752  0.0040438757
## ..     ...         ...                ...        ...           ...
\end{verbatim}
}

The output reveals that in a crisis year during the second era for
example total assets grew by 1.2 percent less than during non-crisis
years. We can double check that by manually subtracting the non crisis
value 0.1076 from the first crisis value 0.0953 (that gives us -0.0123,
which can also be found column `Dmeans'). Similarly, subtract it from
the other values to double check those results, too.

Since the values for non-crisis years have already cancelled themselves
out, we can go straight on to sum the values up. As before, we will be
using the function \emph{summarise()} for that.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cumlog =}\StringTok{ }\KeywordTok{summarise}\NormalTok{(tmp.cumlogD, }\DataTypeTok{cumulativeEffects =} \KeywordTok{sum}\NormalTok{(Dmeans)) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(variable, }\KeywordTok{desc}\NormalTok{(era)) }\CommentTok{# reorder for reasons of clarity}

\NormalTok{cumlog}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [6 x 3]
## Groups: variable
## 
##             variable     era cumulativeEffects
## 1 D log(Bank Assets)  PreWW2       -0.13126778
## 2 D log(Bank Assets) PostWW2       -0.19668089
## 3  D log(Bank Loans)  PreWW2       -0.21495347
## 4  D log(Bank Loans) PostWW2       -0.11805178
## 5 D log(Broad Money)  PreWW2       -0.13731636
## 6 D log(Broad Money) PostWW2       -0.04597918
\end{verbatim}
}

We see that pre to after crisis results show a reduction in the
cumulative level effects for total bank loans and broad money. In
contrast to this, the cumulative effects on total bank assets were
actually even higher in the post World War 2 era, they increased by more
than six percent.

If we compare this to the graph generated in the first part of this
exercise, on which it seems as if the effects within the after crisis
era were smaller than in the first era, we realise that the graph is a
bit misleading here. From the numbers we can deduce that the effects of
crises in the second era had a bigger impact than we might have thought
from a look at the graph. Of course, in the second era the overall
growth rate is higher and the variables remain at a higher level than in
the first era, so it remains debatable which impact is to be rated more
severe.

The other two variables experience a great reduction in the cumulative
effects. The effects on broad money were significantly reduced, the
cumulative level effect decreased from around 14 to only 4.5 percent,
and although the consequences from crises on bank loans were not brought
as close to zero, the numbers have still decreased by roughly ten
percent. The reduction of total post crisis effects on bank loans but
not on bank assets might be caused by the fact that deposit insurances
were launched as a result of lessons learned from the great depression
and therefore became effective in the second era. For bank assets,
Schularick and Taylor offer the possible explanation that bank assets
remained uninsured.

\subsubsection{\sf Exercise 2.2 -- Crisis Effects on Price
Development}\label{exercise-2ux5f2-crisis-effects-on-price-development}

In this part of the exercise, you will be guided through additional
steps which will teach you how to set up the data before you can plot
them. Therefore we will load the data set provided by Schularick and
Taylor first, modify it and create all variables we need for the plot.
Then we will convert the data into long format, build the aggregates of
the price development and finally plot them.

Run the following chunk to read in the data set.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(foreign)}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.dta}\NormalTok{(}\StringTok{"panel17m.dta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

From this data set, we will extract six variables which we will work
with in the following: \textbf{year} and \textbf{iso} which are the
index variables of the panel data set, variable \textbf{crisisST} which
is the dummy variable indicating when a crisis happened, the variables
containing the broad and narrow money aggregates, \textbf{money} and
\textbf{narrowm} and the variable which contains the consumer price
index, \textbf{cpi}.

As mentioned in the previous part of the exercise, we can only compare
the growth rates of the data variables, so while we extract them we can
directly convert them by using the function \emph{transmute()} from the
package \texttt{dplyr}. Also, because of this conversion we need to
group the data to treat each country as a separate group when we are
subtracting data with \emph{diff()}, which has to be done in order to
calculate the growth rates. Apart from the three growth rate variables,
we also create the two index variables that are being used for the plot
later on. You saw them in the previous part of the exercise, too.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{# data preparation: creation of new variables}
\CommentTok{# use grouped data to cope with the specifics of panel data }
\NormalTok{tmp.prep3 =}\StringTok{ }\KeywordTok{transmute}\NormalTok{(}\KeywordTok{group_by}\NormalTok{(data, iso), }
                      
                      \CommentTok{# create the growth rates; }
                      \CommentTok{# 'diff()' returns n-1 values, thus we pad the vector with NA}
                      \StringTok{"D log(Broad money)"} \NormalTok{=}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, (}\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(money)))), }
                      \StringTok{"D log(Narrow money)"} \NormalTok{=}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, (}\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(narrowm)))),}
                      \StringTok{"D log(CPI)"} \NormalTok{=}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, (}\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(cpi)))),}
                      
                      \CommentTok{# was there a crisis in the preceeding five years?}
                      \DataTypeTok{afterCrisis =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{compute.dist}\NormalTok{(crisisST)), }
                      
                      \CommentTok{# 'era' divides the data into the two eras}
                      \DataTypeTok{era =} \KeywordTok{ifelse}\NormalTok{(year <=}\StringTok{ }\DecValTok{1945}\NormalTok{,  }\StringTok{"PreWW2"}\NormalTok{, }\StringTok{"PostWW2"}\NormalTok{),}
                      
                      \CommentTok{#needed to filter out the war years in the next step}
                      \NormalTok{year}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

If you like, you can have a look at the data now in the data explorer,
just click \texttt{data} to enter it. The index variable \textbf{era}
indicates in which era (cf.~exercise 1) a data value has been observed.
The second index variable for the later plot is \textbf{afterCrisis}. It
indicates the number of years since the last crisis happened. This is
done for the first five years after the crisis, assuming that then the
effects of a crisis have vanished. If no crisis happened within the last
five years, this variable is given the default value of `-1'. The
variable \textbf{afterCrisis} will be a factor variable, it is
calculated by the implicitly loaded user-defined function
\emph{compute.dist()}. An example of the exact behaviour of
\emph{compute.dist()} can be found in the next info block. Note that in
the original paper there are two errors in the creation of the variable
\textbf{afterCrisis}. Thus the results will differ from our results
here. For more info on this detail, you may consult the next info block.

\begin{infobox2}
\textbf{Info: The error in the creation of the variable 'sample' in the Stata code}\\
{\linespread{0.9}
The panel data set at hand is ordered by countries and then by years.
This means, with $n$ observations per country the data set is ordered in
such a way that we first get all $n$ observations from country 1, then
from country 2, and so on.

The variable \textbf{afterCrisis} corresponds to the variable
\textbf{sample} in the original Stata code. These two variables are
supposed to indicate how many years ago (up to 5 years) the last crisis
happened in a country. For a crisis more than five years in the past
this is indicated by \texttt{-1} or \texttt{normal}. Say, if a crisis
happened in 1990, \textbf{sample} in Stata and \textbf{afterCrisis} in
our data set would be 1 in 1991, 2 in 1992,.., 5 in 1995, and -1 in
1996.

In this exercise, \textbf{afterCrisis} is created by
\emph{compute.dist()}. Let us demonstrate the function: First the basic
behaviour is demonstrated, and in part 2 it is shown how to use it on
some random sample panel data with three index categories.
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# part 1}
\CommentTok{# set up a sample vector for this example}
\NormalTok{test =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{) }
\NormalTok{test }\CommentTok{# print vector}
\end{Highlighting}
\end{Shaded}
\vskip -3em \quad
{\small
\begin{verbatim}
##  [1] 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1
\end{verbatim}
}
\vskip -3em \quad
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{compute.dist}\NormalTok{(test)  }\CommentTok{# print result from 'compute.dist()'on test}
\end{Highlighting}
\end{Shaded}
\vskip -3em \quad
{\small
\begin{verbatim}
##  [1] -1  0  1  2  3  4  5 -1 -1  0  1  0  1  2  0
\end{verbatim}
}
\vskip -3em \quad
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# part 2}
\CommentTok{# extend sample with categories (sample panel data)}
\NormalTok{test2 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{cat =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{), }\DataTypeTok{each =} \DecValTok{5}\NormalTok{), test) }
\KeywordTok{mutate}\NormalTok{(}\KeywordTok{group_by}\NormalTok{(test2, cat), }\KeywordTok{compute.dist}\NormalTok{(test))}
\end{Highlighting}
\end{Shaded}

\begin{SAMEPAGE}
\linespread{0.9}
{\small
\begin{verbatim}
## Source: local data frame [15 x 3]
## Groups: cat
## 
##    cat test compute.dist(test)
## 1    1    0                 -1
## 2    1    1                  0
## 3    1    0                  1
## 4    1    0                  2
## 5    1    0                  3
## 6    2    0                 -1
## 7    2    0                 -1
## 8    2    0                 -1
## 9    2    0                 -1
## 10   2    1                  0
## 11   3    0                 -1
## 12   3    1                  0
## 13   3    0                  1
## 14   3    0                  2
## 15   3    1                  0
\end{verbatim}
}
\end{SAMEPAGE}

\vskip -2em However, Schularick and Taylor made a mistake at part 2 in the
implementation of this example. At the transition points between the
countries they drag the after crisis years into the next countries data.
The following code chunk faithfully replicates this incorrectly
implemented original variable \textbf{sample} from Schularick and
Taylor's Stata code.

First we replicate their code for the computation of the variable
\textbf{sample}. Then we are going to compare it with the corrected
version which is stored in the variable \textbf{afterCrisis}. In
particular, we compare the transition between Sweden and the USA in the
data of the paper.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# declare vector sample, all values -1}
\NormalTok{sample =}\StringTok{ }\KeywordTok{rep}\NormalTok{(-}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(data$crisisST))}

\CommentTok{# add to indicate crises with zero under certain conditions}
\NormalTok{sample[data$crisisST >=}\StringTok{ }\DecValTok{1} \NormalTok{&}
	\NormalTok{(}\KeywordTok{lag}\NormalTok{(data$crisisST) ==}\StringTok{ }\DecValTok{0} \NormalTok{|}\StringTok{ }\KeywordTok{is.na}\NormalTok{(}\KeywordTok{lag}\NormalTok{(data$crisisST)))] <-}\StringTok{ }\DecValTok{0}

\CommentTok{# indicate following years as post crisis (up to 5)}
\NormalTok{sample[}\KeywordTok{lag}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(sample)) %in%}\StringTok{ }\DecValTok{0}\NormalTok{] <-}\StringTok{ }\DecValTok{1} 
\NormalTok{sample[}\KeywordTok{lag}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(sample)) %in%}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{2}
\NormalTok{sample[}\KeywordTok{lag}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(sample)) %in%}\StringTok{ }\DecValTok{2}\NormalTok{] <-}\StringTok{ }\DecValTok{3}
\NormalTok{sample[}\KeywordTok{lag}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(sample)) %in%}\StringTok{ }\DecValTok{3}\NormalTok{] <-}\StringTok{ }\DecValTok{4}
\NormalTok{sample[}\KeywordTok{lag}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(sample)) %in%}\StringTok{ }\DecValTok{4}\NormalTok{] <-}\StringTok{ }\DecValTok{5}

\CommentTok{# Comparison of the mistaken and the corrected versions}
\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, iso, year, crisisST), }
           \NormalTok{sample, }
           \DataTypeTok{afterCrisis =} \NormalTok{tmp.prep3$afterCrisis)[}\DecValTok{1805}\NormalTok{:}\DecValTok{1830}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{SAMEPAGE}
{\small
\begin{verbatim}
##      iso year crisisST sample afterCrisis
## 1805 SWE 2006        0     -1          -1
## 1806 SWE 2007        0     -1          -1
## 1807 SWE 2008        1      0           0
## 1808 USA 1870        0      1          -1
## 1809 USA 1871        0      2          -1
## 1810 USA 1872        0      3          -1
## 1811 USA 1873        1      4           0
## 1812 USA 1874        0      5           1
## 1813 USA 1875        0      2           2
## 1814 USA 1876        0      3           3
## 1815 USA 1877        0      4           4
## 1816 USA 1878        0      5           5
## 1817 USA 1879        0     -1          -1
## 1818 USA 1880        0     -1          -1
## 1819 USA 1881        0     -1          -1
## 1820 USA 1882        0     -1          -1
## 1821 USA 1883        0     -1          -1
## 1822 USA 1884        1      0           0
## 1823 USA 1885        0      1           1
## 1824 USA 1886        0      2           2
## 1825 USA 1887        0      3           3
## 1826 USA 1888        0      4           4
## 1827 USA 1889        0      5           5
## 1828 USA 1890        0     -1          -1
## 1829 USA 1891        0     -1          -1
## 1830 USA 1892        0     -1          -1
\end{verbatim}
}
\end{SAMEPAGE}

\vskip -2em For example, you can see that the crisis in Sweden in 2008, indicated by
\texttt{0} in \textbf{sample} and \textbf{afterCrisis} and \texttt{1} in
\textbf{crisisST}, leads to the years 1870 -- 1874 being mistakenly treated
as post crisis years in the USA in \textbf{sample} -- although in fact a
new crisis is indicated in 1873 by \textbf{crisisST}. This second crisis
is not handled correctly in the original code.

Since the demonstrated behaviour from the original analysis makes little
sense, in this problem set we will proceed according to the supposed
intentions of the authors and therefore will be using the corrected
variable \textbf{afterCrisis}. Differences in the results and in the
following plots are due to this correction. The code chunks have been
tested, however, with \textbf{sample} instead of \textbf{afterCrisis}
enabling us to replicate the original results from the Stata code.
\end{infobox2}

In the next code chunk, we prepare the data a bit. We use another useful
function from the package \texttt{dplyr}, the function \emph{filter()}.
It allows, to filter out the war years and, similar to Schularick and
Taylor, we remove years after-war years which are possibly still
affected from the war.

Notice that values in \textbf{afterCrisis} not subsequent to a crisis
were labeled as \texttt{-1} by the function \emph{compute.dist()}. As a
preparation of the plot we are going to relabel them to `Normal' to
obtain a more readable form of the x-ticks in the bar charts later on.
Since these values are treated as categories (factors), relabeling does
not change anything else in our calculations.

Thirdly we remove the grouping variable (thus we have to ungroup the
data frame) which was needed in the first step to calculate the new
series according to each country. Since we do not need it anymore, we
remove the variable \textbf{year} from the data, too.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove war years}
\NormalTok{tmp.prep3_wow =}\StringTok{ }\KeywordTok{filter}\NormalTok{(tmp.prep3, !year %in%}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1914}\NormalTok{, }\DataTypeTok{to =} \DecValTok{1919}\NormalTok{), }
                                  \NormalTok{!year %in%}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1939}\NormalTok{, }\DataTypeTok{to =} \DecValTok{1947}\NormalTok{))}

\CommentTok{# replace the factor '-1' in 'afterCrisis' by 'Normal'}
\KeywordTok{levels}\NormalTok{(tmp.prep3_wow$afterCrisis)[}\DecValTok{1}\NormalTok{] =}\StringTok{ "Normal"}

\CommentTok{# remove grouping / variable iso}
\NormalTok{tmp.prep3_df =}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{ungroup}\NormalTok{(tmp.prep3_wow), -iso, -year)}
\KeywordTok{head}\NormalTok{(tmp.prep3_df, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [3 x 5]
## 
##   D log(Broad money) D log(Narrow money)  D log(CPI) afterCrisis    era
## 1                 NA                  NA          NA      Normal PreWW2
## 2          0.0914521           0.1547637 -0.01550395      Normal PreWW2
## 3          0.1408574           0.2858421 -0.04800921      Normal PreWW2
\end{verbatim}
}

If you have done the previous part of the exercise, this output should
already look familiar to you. The difference is that the data is yet in
wide format, which is why we have separate columns per data variable.
With the function \emph{melt()} from the package \texttt{reshape2} that
we already used in preceding exercises, we can convert the data frame
into wide format. All we have to do is to specify the data we want to
melt and which variables within the data should remain as ID-variables.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2)}
\CommentTok{# melt data}
\NormalTok{tmp.molten3 =}\StringTok{ }\KeywordTok{melt}\NormalTok{(}\DataTypeTok{data =} \NormalTok{tmp.prep3_df, }\DataTypeTok{id.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"afterCrisis"}\NormalTok{, }\StringTok{"era"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(tmp.molten3, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##   afterCrisis    era           variable     value
## 1      Normal PreWW2 D log(Broad money)        NA
## 2      Normal PreWW2 D log(Broad money) 0.0914521
## 3      Normal PreWW2 D log(Broad money) 0.1408574
\end{verbatim}
}

Now we are basically one step away from being able to create the plot
that we are interested in. All that is left to do is to create the
aggregated mean values. The calculation of these mean values can be done
easily with the function \emph{summarise()}. It is now your turn to:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  group the data by the variables \textbf{era}, \textbf{afterCrisis},
  and \textbf{variable} (assign the result to \textbf{tmp.grouped3}),
  and then
\item
  to pass the grouped data to \emph{summarise()}. Recall that you can
  use aggregate functions as additional parameter in which you specify a
  column of the data object. The aggregate function will then calculate
  its results for every group and return one value per group.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmp.grouped3 =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(tmp.molten3, era, afterCrisis, variable)}
\CommentTok{# Uncomment the lines and adapt what is missing}
\CommentTok{# tmp.grouped3 = ?}
\CommentTok{# # Using the function summarise(), calculate the means over 'value'}
\CommentTok{# tmp.prep.fig6=  ?( ? , means = ?( ? , na.rm = TRUE))}
\NormalTok{tmp.prep.fig6 =}\StringTok{ }\KeywordTok{summarise}\NormalTok{(tmp.grouped3, }\DataTypeTok{means =} \KeywordTok{mean}\NormalTok{(value, }\DataTypeTok{na.rm = }\NormalTok{T))}
\end{Highlighting}
\end{Shaded}

We have now reached the point at which we can plot the bar chart (this
is the point at which we started in the first part of this exercise).
The call is similar to the one we used to generate the first bar chart
in part one, the only difference being that the parameters `data' and
`main' are adapted to the new data. You can now check the code chunk to
create the bar chart.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barchart}\NormalTok{(means ~}\StringTok{ }\NormalTok{afterCrisis |}\StringTok{ }\NormalTok{era,    }\CommentTok{# y ~ x, plot data for each category in }
                                       \CommentTok{# 'era' separately into a different panel}
           \DataTypeTok{groups =} \NormalTok{variable,         } \CommentTok{# one bar per variable }
           \DataTypeTok{index.cond =} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)), }\CommentTok{# change order of panels}
           \DataTypeTok{data = }\NormalTok{tmp.prep.fig6,       }\CommentTok{# data selection}
           \DataTypeTok{main = }\StringTok{"Money and Inflation"}\NormalTok{,}
           \DataTypeTok{origin =} \DecValTok{0}\NormalTok{,                 }\CommentTok{# changes appearance of bars}
           \DataTypeTok{auto.key =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{space =} \StringTok{"bottom"}\NormalTok{, }\DataTypeTok{columns =} \DecValTok{3}\NormalTok{), }\CommentTok{# change legend}
           \DataTypeTok{par.settings =} \KeywordTok{simpleTheme}\NormalTok{(}\DataTypeTok{col =} \KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Blues"}\NormalTok{)) }\CommentTok{# change design}
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/2_2___43-1.pdf}

\vskip 1em We have illustrated the development of broad money in comparison to
total bank assets and total bank loans in the previous bar chart, where
we saw that the impact of a crisis on money and loans was reduced
greatly in the postwar era, possibly due to increased support of central
banks. In addition to that we can now see that in the postwar era narrow
money even shows some compensation effects in the first year after a
crisis. On the other hand narrow money is fluctuating in the first years
in and after a crisis. The chart therefore suggests that it is impacted
negatively by a crisis as well. We will double check this result once we
have calculated the cumulative level effects.

The behaviour of the price index in the first era is also very striking.
Showing low levels of inflation in normal times, the negative values in
the post crisis years mean that prices go down in the years following a
crisis. This deflation is even five years after a crisis still
noticeable. In contrast, in the second era the price index is initially
driven in the opposed direction, thus we are dealing with higher price
rises as result of a crisis in the following years.

To see how high the overall impact on the first five years after a
crisis was, we are going to sum up again the differences in the growth
rates relative to normal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# group by 'variable' and 'era' }
\NormalTok{tmp3.cumlog =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(tmp.prep.fig6, variable, era)}

\CommentTok{# creation of differences between non crisis and post crisis years for each group}
\NormalTok{tmp3.cumlogD =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(tmp3.cumlog, }\DataTypeTok{Dmeans =} \NormalTok{means - }\KeywordTok{head}\NormalTok{(means, }\DataTypeTok{n =} \DecValTok{1}\NormalTok{))}

\CommentTok{# compute cumulative level effects, then rearrange it}
\KeywordTok{summarise}\NormalTok{(tmp3.cumlogD, }\DataTypeTok{cumulativeEffects =} \KeywordTok{sum}\NormalTok{(Dmeans)) %>%}
\KeywordTok{arrange}\NormalTok{(variable, }\KeywordTok{desc}\NormalTok{(era))}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [6 x 3]
## Groups: variable
## 
##              variable     era cumulativeEffects
## 1  D log(Broad money)  PreWW2      -0.137316364
## 2  D log(Broad money) PostWW2      -0.045979181
## 3 D log(Narrow money)  PreWW2      -0.085894912
## 4 D log(Narrow money) PostWW2       0.003201071
## 5          D log(CPI)  PreWW2      -0.094828378
## 6          D log(CPI) PostWW2       0.036910267
\end{verbatim}
}

The results reveal that both narrow money and inflation, in the first
era yet significantly suffering from reduced growth rates as reaction to
a crisis, show now positive cumulative level effects. Narrow money
almost unaffected. That proves that the compensation effect in the first
year after a crisis, which is probably driven by central bank and
government actions and their more aggressive monetary policy responses,
is strong enough to reduce the impacts of a crisis on narrow money to a
minimum in terms of growth. The figure for the price index even shows
that the inflation was over the five year period roughly three percent
higher relative to normal periods in the second era, which is a huge
leap when we compare it to the value from the prewar era, which
indicates an overall reduction of price levels by nearly 10 percent
during the five years altogether.

\subsubsection{\sf Exercise 2.3 -- Crisis Effects on Real
Economy Development}\label{exercise-2ux5f3-crisis-effects-on-real-economy}

In this last part of the exercise, a third aftermath analysis will be
conducted. This time we are interested in the crisis effects on the real
economy. This part of the third exercise will be based on what you have
learned in the preceding parts. For some steps, you will either have to
answer multiple choice questions with one or more correct answers, or
you have to do the coding yourself. If a question has more than one
correct answer, enter it as a character vector (e.g.~c(``A'',``C'') ).

The real economy effects we are analysing will be the effects of crises
on real GDP and real investment. Again, our primary aim is to plot a bar
chart. Before we can start, we must load the data set one more time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package 'foreign', then the data set}
\KeywordTok{library}\NormalTok{(foreign)}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.dta}\NormalTok{(}\StringTok{"panel17m.dta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The first question will test your understanding of the creation of the
variables for the later plot.

\textbf{Question 1:} We have said previously that we cannot compare the
same data variables directly among each country, due to different units
and currencies. We can however compare the growth rates, which we will
create in the first step. To calculate the growth rates, we used the
function \emph{diff()} before. What was and is the difficulty with
\emph{diff()} in our context?

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A: We must specify a parameter indicating the order of difference
\item
  B: \emph{diff()} returns a vector that has a different size than its
  input vector
\item
  C: \emph{diff()} does not distinguish between data from different
  countries
\end{itemize}

Pass the answer to \textbf{Answer\_1}. If you need a hint, you can
either click \texttt{hint}, or you can cheat by looking into the code
chunk in which we will create the variables for this part of the
exercise. At the moment the code chunk is hidden but you can unfold it
with a click on `Hidden Code 1' below the present code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pass the correct answer as string to 'Answer_1' and uncomment it}
\CommentTok{# Answer_1 = ???}
\NormalTok{Answer_1 =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\infobox{Info: Answer to Question1}{

Although we could specify a parameter to indicate the order of
difference, this is not mandatory. Thus answer ``A'' is wrong.

Answer ``B'' is correct. Function \emph{diff()} actually returns a
vector that is by one element shorter than its input. We need the output
to be of the same length as the input, and we overcome this problem by
padding the output with an NA value in position one.

Answer ``C'' is also correct. We can come around this problem by using
\emph{diff()} within \emph{mutate()} or within \emph{transmute()} if we
pass the data in a grouped form to these two functions.
}

You now have to click on `Hidden Code 1', then check the appearing code
chunk.

\begin{infobox2}
\textbf{Hidden Code 1}

In the creation of the variables, note that `Real Investment' is set up
as the sum of the logarithmic values of investment per year and real
GDP.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{# data preparation: creation of new variables}
\CommentTok{# group data to cope with the specifics of panel data }
\NormalTok{tmp.prep2 =}\StringTok{ }\KeywordTok{transmute}\NormalTok{(}\KeywordTok{group_by}\NormalTok{(data, iso), }
                \StringTok{"D log(Real GDP)"} \NormalTok{=}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, (}\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(rgdp)))), }
                \StringTok{"D log(Real Investment)"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, (}\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(iy) +}\StringTok{ }\KeywordTok{log}\NormalTok{(rgdp)))),}
                \DataTypeTok{afterCrisis =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{compute.dist}\NormalTok{(crisisST)), }
                \DataTypeTok{era =} \KeywordTok{ifelse}\NormalTok{(year <=}\StringTok{ }\DecValTok{1945}\NormalTok{,  }\StringTok{"PreWW2"}\NormalTok{, }\StringTok{"PostWW2"}\NormalTok{),}
                \NormalTok{year}
            \NormalTok{) }

\CommentTok{# remove war years}
\NormalTok{tmp.prep2_wow =}\StringTok{ }\KeywordTok{filter}\NormalTok{(tmp.prep2, !year %in%}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1914}\NormalTok{, }\DataTypeTok{to =} \DecValTok{1919}\NormalTok{), }
                                  \NormalTok{!year %in%}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1939}\NormalTok{, }\DataTypeTok{to =} \DecValTok{1947}\NormalTok{))}

\CommentTok{# replace the factor '-1' in 'afterCrisis' by 'Normal'}
\KeywordTok{levels}\NormalTok{(tmp.prep2_wow$afterCrisis)[}\DecValTok{1}\NormalTok{] =}\StringTok{ "Normal"}

\CommentTok{# remove grouping / variable iso}
\NormalTok{tmp.prep2_df =}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{ungroup}\NormalTok{(tmp.prep2_wow), -iso, -year)}
\KeywordTok{head}\NormalTok{(tmp.prep2_df, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [3 x 4]
## 
##   D log(Real GDP) D log(Real Investment) afterCrisis    era
## 1              NA                     NA      Normal PreWW2
## 2     0.007689945            -0.03614799      Normal PreWW2
## 3     0.074441953             0.29539626      Normal PreWW2
\end{verbatim}
}
\end{infobox2}
\COMMENT{Hidden code is only concealed to not anticipate the answer to the Question that the user has to solve. 
It can be used as additional hint, however it must be solved before the user can proceed with the following
tasks.}

Again we want to plot more than one series at the same time, thus we
need to melt the data into long format. Remember that when you melt
variables you take two or more variables which were in separate columns
before and concatenate them all into one column, which is the reason why
that format is called long format.

This is supposed to be done by you. You should do it in the same way as
it has been done in preceding code. Specify the two arguments `data' and
`id.vars' and assign the function call to \textbf{tmp.molten2}. To check
the resulting data frame, print the first four values of
\textbf{tmp.molten2}. As a hint: the ID-variables are usually
categorical variables and do not get melted themselves.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# melt data}
\NormalTok{tmp.molten2 =}\StringTok{ }\KeywordTok{melt}\NormalTok{(}\DataTypeTok{data =} \NormalTok{tmp.prep2_df, }\DataTypeTok{id.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"afterCrisis"}\NormalTok{, }\StringTok{"era"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(tmp.molten2, }\DataTypeTok{n =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\vskip -3em \quad
{\small
\begin{verbatim}
##   afterCrisis    era        variable       value
## 1      Normal PreWW2 D log(Real GDP)          NA
## 2      Normal PreWW2 D log(Real GDP) 0.007689945
## 3      Normal PreWW2 D log(Real GDP) 0.074441953
## 4      Normal PreWW2 D log(Real GDP) 0.073287731
\end{verbatim}
}

Look at the four columns. The first two columns are the ones you defined
as ID-variables, the third contains the names of the variables you
melted and the fourth one the respective values.

\textbf{Question 2:} Before we can plot the mean values of the data, we
have to compute them first for groups we specify with the ID-variables.
Which function should we employ, when we want to use functions like
\emph{sum()} or \emph{mean()} (aggregate functions) on different groups?

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  A: select()
\item
  B: summary()
\item
  C: summarise()
\item
  D: mutate()
\item
  E: transmute()
\end{itemize}

Pass the answer to \textbf{Answer\_2}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pass the correct answer as string to 'Answer_2'}
\CommentTok{# Answer_2 = ???}
\NormalTok{Answer_2 =}\StringTok{ "C"}
\end{Highlighting}
\end{Shaded}

\infobox{Info: Answer to Question 2}{
The correct answer is ``C''.

The function \emph{summarise()} passes all values of each group together
but each group separately to the aggregate function and gets one value
per group in return. As we saw before, \emph{mutate()} and
\emph{transmute()} work similarly -- they also pass the data per group
to a function, but the function has to return the same amount of values
as were passed to it. \emph{select()} only selects (or excludes, as in
this exercise) variables from a data frame and \emph{summary()} might
sound similar, but is actually a generic function which has different
tasks in different settings.
}

To not disclose the answer, the code chunk in which we do the
calculation of the means has been hidden. After answering the question,
reveal it by clicking on ``Hidden Code 2''.

\begin{infobox2}
\textbf{Hidden Code 2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# specify correct grouping order (as preparation to summarise correctly)}
\NormalTok{tmp.grouped2 =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(tmp.molten2, era, afterCrisis, variable)  }

\CommentTok{# calculate aggregated values}
\NormalTok{tmp.prep.fig5 =}\StringTok{ }\KeywordTok{summarise}\NormalTok{(tmp.grouped2, }\DataTypeTok{means =} \KeywordTok{mean}\NormalTok{(value, }\DataTypeTok{na.rm =} \NormalTok{TRUE))}

\CommentTok{# display the aggregates (tmp.prep.fig5 is grouped, so output is limited)}
\NormalTok{tmp.prep.fig5}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [28 x 4]
## Groups: era, afterCrisis
## 
##        era afterCrisis               variable         means
## 1  PostWW2      Normal        D log(Real GDP)  0.0282220409
## 2  PostWW2      Normal D log(Real Investment)  0.0309109199
## 3  PostWW2           0        D log(Real GDP)  0.0081895434
## 4  PostWW2           0 D log(Real Investment) -0.0004186928
## 5  PostWW2           1        D log(Real GDP)  0.0036941797
## 6  PostWW2           1 D log(Real Investment) -0.0536371612
## 7  PostWW2           2        D log(Real GDP)  0.0106584446
## 8  PostWW2           2 D log(Real Investment) -0.0378683662
## 9  PostWW2           3        D log(Real GDP)  0.0211451615
## 10 PostWW2           3 D log(Real Investment) -0.0062817126
## ..     ...         ...                    ...           ...
\end{verbatim}
}
\end{infobox2}

Because \textbf{tmp.prep.fig5} remains grouped after the application of
\emph{summarise()}, the output is again limited to ten observations.
(You might notice that we grouped by three variables, but now it is only
grouped by two variables. This is not a bug -- the function
\emph{summarise()} strips off one group in reverse order, so we could
theoretically use another aggregate function on the remaining data). We
could interpret these values now, but you know by now that it is a lot
easier to grasp the results when we create a bar chart again.

The function \emph{barchart()} expects a formula as input, similar to
\emph{lm()}. As with \emph{lm()}, you also assign what you want to plot
on the x-axis and the y-axis and you separate it with the symbol
\texttt{\textasciitilde{}}. The part behind the symbol
\texttt{\textbar{}} is a condition that you can define. It is helpful,
when your data comprises categorical variables. Based on the categories,
\emph{barchart()} will create different plots.

Previously, you melted the data. As a result, all observations in the
data, irrespective of which variables they belong to, are now saved in
one column and the corresponding variable names are saved in a second
column. You can think of this second column as another factor variable
with the names of the variables being the groups, for which
\emph{barchart()} will create separate bars if you assign the variable
containing the variable names to the parameter `groups'.

Most of the creation of the bar chart has been prepared for you. To
actually create the bar chart now, replace all `???' and uncomment the
lines.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lattice)}
\CommentTok{# barchart( ??? ~ ??? | ???,   # y ~ x for each category in variable behind '|'}
\CommentTok{#                              # --> two categories in era: plot separately into}
\CommentTok{#                              #     two panels}
\CommentTok{#            groups = ??? ,                           # one bar per variable }
\CommentTok{#            index.cond = list(c(2,1)),               # change order of panels}
\CommentTok{#            data = ??? ,                             # data selection}
\CommentTok{#            main = "Real Variables",}                # set up title
\CommentTok{#            origin = 0,                              # changes appearance of bars}
\CommentTok{#            auto.key = list(space = "bottom", columns = 3),          # change legend}
\CommentTok{#            par.settings = simpleTheme(col = brewer.pal(3, "Blues")) # change design}
\CommentTok{#   )}
\KeywordTok{barchart}\NormalTok{(means ~}\StringTok{ }\NormalTok{afterCrisis |}\StringTok{ }\NormalTok{era, }
           \DataTypeTok{groups =} \NormalTok{variable,       }
           \DataTypeTok{index.cond =} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)), }
           \DataTypeTok{data =} \NormalTok{tmp.prep.fig5,      }
           \DataTypeTok{main =} \StringTok{"Real Variables"}\NormalTok{,}
           \DataTypeTok{origin =} \DecValTok{0}\NormalTok{,              }
           \DataTypeTok{auto.key =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{space =} \StringTok{"bottom"}\NormalTok{, }\DataTypeTok{columns =} \DecValTok{3}\NormalTok{),}
           \DataTypeTok{par.settings =} \KeywordTok{simpleTheme}\NormalTok{(}\DataTypeTok{col =} \KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Blues"}\NormalTok{)) }
  \NormalTok{)}
\end{Highlighting}
\end{Shaded}


\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/2_3__51-1.pdf}

Comparing the effects of the two eras gives a different picture than the
plot before. According to this analysis, the effects of a crisis on the
real economy were not much different before and after the Second World
War. The peak influence on real investment seems to happen one year
after a crisis. Where in the first era real investment recovers after
four years to levels higher than during normal times and thus
compensates for the recession after a crisis, this compensation effect
is not noticeable for the post World War 2 era. Similar to the behaviour
of real investment, the peak influence on real GDP is also observed on
average one year after a crisis. However, the effects are much smaller
and basically do not differ for pre and after crisis times.

Turning again towards the average total impact of a crisis on the
development of the respective variables we are going to calculate the
cumulative level effects, which means we will have to sum up by how much
in total the series were affected in the first five years after a
crisis.

For the calculation of the cumulative level effects we have to group our
data and then calculate the differences between after-crisis years and
normal years again. Recall that within each group normal years are saved
at position one, so you can subtract them for example with the function
\emph{head()} for which you could use the argument $n = 1$.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# group by 'variable' and 'era' }
\NormalTok{tmp2.cumlog =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(tmp.prep.fig5, variable, era)}

\CommentTok{# creation of differences between non crisis and post crisis years for each group}
\NormalTok{tmp2.cumlogD =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(tmp2.cumlog, }\DataTypeTok{Dmeans =} \NormalTok{means-}\KeywordTok{head}\NormalTok{(means, }\DataTypeTok{n =} \DecValTok{1}\NormalTok{))}
\NormalTok{tmp2.cumlogD}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [28 x 5]
## Groups: variable, era
## 
##        era afterCrisis               variable         means       Dmeans
## 1  PostWW2      Normal        D log(Real GDP)  0.0282220409  0.000000000
## 2  PostWW2      Normal D log(Real Investment)  0.0309109199  0.000000000
## 3  PostWW2           0        D log(Real GDP)  0.0081895434 -0.020032498
## 4  PostWW2           0 D log(Real Investment) -0.0004186928 -0.031329613
## 5  PostWW2           1        D log(Real GDP)  0.0036941797 -0.024527861
## 6  PostWW2           1 D log(Real Investment) -0.0536371612 -0.084548081
## 7  PostWW2           2        D log(Real GDP)  0.0106584446 -0.017563596
## 8  PostWW2           2 D log(Real Investment) -0.0378683662 -0.068779286
## 9  PostWW2           3        D log(Real GDP)  0.0211451615 -0.007076879
## 10 PostWW2           3 D log(Real Investment) -0.0062817126 -0.037192633
## ..     ...         ...                    ...           ...          ...
\end{verbatim}
}

If you want to, you can manually double check the results again by
subtracting the correct values from each other in column `means' to get
the values in column `Dmeans'.

In a last step you can now calculate the cumulative level effects by
summation of the correct values in variable \textbf{Dmeans}. As before,
we will be using the function \emph{summarise()} for that. Since we
grouped the data previously, it will compute the appropriate values for
each group. You should name the column of the summed values of
\textbf{Dmeans} `cumulativeEffects'.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(tmp2.cumlogD, }\DataTypeTok{cumulativeEffects =} \KeywordTok{sum}\NormalTok{(Dmeans)) }
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Source: local data frame [4 x 3]
## Groups: variable
## 
##                 variable     era cumulativeEffects
## 1        D log(Real GDP) PostWW2       -0.07709132
## 2        D log(Real GDP)  PreWW2       -0.02822617
## 3 D log(Real Investment) PostWW2       -0.22658465
## 4 D log(Real Investment)  PreWW2       -0.18698793
\end{verbatim}
}

From the preceding graph we already saw that there was not much
improvement from pre to postwar era in the development of the two
variables. In fact, from the cumulative level effects we can even deduce
that both real GDP and real investment show a higher total reduction in
growth over the five after crisis years in the post World War 2 era than
was observed in the era before. For both series, the increase in the
numbers is roughly five percent.

Schularick and Taylor note that the prewar output decline effect is to a
huge extent caused by the great depression and they reason this
inference with the argument that in the years before the Thirties,
economy was not as much influenced by the financial sector. Since then
the financial sector has grown widely and became a lot more leveraged.
This could be a possible explanation as to why efforts from governments
with changes in policies to prevent crisis having too much impact on the
real economy could have failed to overcome the newly arisen risks. As
mentioned before, after the Thirties policies have been changed towards
preventing bank runs by stronger depositor protection, but as
Demirgüç-Kunt and Huizinga (2004) were able to demonstrate, this nearly
always leads to moral hazard effects, actually even increasing the risk
further. Thus effects on the financial sector might be reduced by those
policies, but on the real economy the implications now increase.

\textbf{Question 3:} We just saw that the overall impact of crises on
both real economy indicators was stronger in the time after World War 2.
What has not changed among the two eras is, according to the results
here, the time until real investments approximately reached the `normal'
level after a crisis. How long did this take?

Pass the answer as integer between 1 and 5 to \textbf{Answer\_3}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pass the correct answer as string to 'Answer_3'}
\CommentTok{# Answer_3 = ???}
\NormalTok{Answer_3 =}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\infobox{Info: Answer to Question 3}{
We recall from the analysis: Real investments shrank in total size in
the first two years of both eras and then needed another year before
they were able to recover to pre crisis levels. Then, however, after
four years they reached those normal levels and in the era before World
War 2, they even exceeded pre crisis levels in the fourth and fifth
year.
}
\eject 

\subsection{\sf Exercise 3 -- Are Financial Crises Predictable?}\label{exercise-3-overview}

The main purpose of this exercise is to evaluate whether credit
aggregates can be used as predictors of the probability of future
financial crises. This question will be analysed for different model
specifications and in two different forms, a linear probability model
and a logit probability model. From these we will derive so called
receiver operating characteristic (ROC) curves. ROC curves measure the
quality of binary classification predictions. The binary classification
here is the question whether there will be a crisis in a certain year in
a certain country.

The Exercise is divided into five parts.

For those who are interested in the detailed replication of the code
from the original paper, Exercise~3.1 offers a possible explication of
the creation of the (lagged) variables.

If you don't need to know about the groundwork and are only interested
in the economic findings, feel free to skip the first part of the exercise
and move directly on to Exercise~3.2, where you find an introduction
into the prediction of financial crises.

In the third part you will use logit regression models for the prediction 
of a crisis and you get to know how to analyse the predictive power of 
the models.

In part four you then evaluate how well the presented models work in actually
forecasting a crisis. In other words, the predictive power of a model will be
examined in further detail.

Finally, in the last part we recall the findings from exercise one and hence 
test the prediction models for subsets of the data corresponding to years from
before and after World War 2 respectively.


\subsubsection{\sf Exercise 3.1 -- Lagged Data}\label{exercise-3ux5f1}

In this part of the exercise, we will set up all variables that we need
later on. We will derive them from the original data set from Schularick
and Taylor, thus initially we will load it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(foreign)}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.dta}\NormalTok{(}\StringTok{"panel17m.dta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

First of all we need to create three variables. They are supposed to
express the differences between the change in total loans, narrow money
and broad money and will be deflated by the consumer price index. These
four variables used for the creation of the three new variables are
logarithmized first to cope with their exponential growth.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create new variables}
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{data =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\KeywordTok{group_by}\NormalTok{(data, iso), }
         \CommentTok{# growth rate cpi (inflation)}
         \DataTypeTok{dlcpi =} \KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(cpi))),}
         \CommentTok{# subtract 'dlcpi' to deflate variables     }
         \DataTypeTok{dlloansr =} \KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(loans1))) -}\StringTok{ }\NormalTok{dlcpi,}
         \DataTypeTok{dlnmr =} \KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(narrowm))) -}\StringTok{ }\NormalTok{dlcpi,}
         \DataTypeTok{dlmr =} \KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(money))) -}\StringTok{ }\NormalTok{dlcpi}
         \NormalTok{)}
\end{Highlighting}
\end{Shaded}

You should now take a quick glance at these variables. Either have a
look in the data explorer by clicking \texttt{data}, which has the
disadvantage that you need to scroll a lot to get to see values for
different countries, or you can check out the next info block.

\begin{infobox2}
\textbf{Info: A quick glance into the new variables}

In the code chunk, we use the operator \texttt{\%\textgreater{}\%}
again. Recall it can be read as \texttt{then} and helps us to use
results without having to save them temporarily. Consequently, the code
can be read as follows: select the variables of interest, then group
these variables by variable \textbf{iso}, then show second and third
entry for each of the groups (the values for 1871 and 1872), then
convert the resulting tbl data frame into a normal data frame and
finally order by year.

Now check the code chunk to see the selected values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print the following variables}
\KeywordTok{select}\NormalTok{(data, year, dlloansr, dlnmr, dlmr) %>%}\StringTok{ }
\CommentTok{# wouldn't be neccessary here, just for demonstration purpose}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(iso) %>%}
\CommentTok{# 2nd and 3rd row per group (--> 1871 and 1872)}
\StringTok{  }\KeywordTok{do}\NormalTok{(.[}\DecValTok{2}\NormalTok{:}\DecValTok{3}\NormalTok{,]) %>%}
\CommentTok{# get rid of grouping (else arrange() doesn't work after do)}
\StringTok{  }\KeywordTok{as.data.frame}\NormalTok{() %>%}\StringTok{ }
\CommentTok{# arrange per year}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(year)  }
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##    iso year     dlloansr       dlnmr        dlmr
## 1  AUS 1871           NA  0.17026762  0.10695605
## 2  CAN 1871  0.178761204          NA          NA
## 3  CHE 1871           NA          NA          NA

   ¦    ¦    ¦            ¦           ¦           ¦

## 26 NOR 1872 -0.008813053  0.05362026  0.04134504
## 27 SWE 1872  0.152490751  0.13211037  0.20646600
## 28 USA 1872           NA  0.00000000          NA
\end{verbatim}
}

As this code reveals, the creation of the three variables was
successful, but at least for the first two years, some countries lack of
data.
\end{infobox2}

In the second part of the exercise we need the data to be available in a
lagged form. In fact we have to compute five series of lags: In the
first we shift the time base back by one time unit, in the second by
two, and so on up to five time units.

Calculating these lags for panel data is again a bit tricky. This has
two major reasons. First of all, we want to remove the war years from
the data in order to prevent our results from being distorted. We have
to be careful at this point, because if we would remove the war years
first and then lag the data, the function we are going to use would not
consider the time gap. Here is an example for the problem that would
arise: data corresponding to year 1913 should be lagged by one, but
would end up seemingly to correspond to the first year after the war,
that is, to 1920. Similarly, it is problematic to remove the war years
after lagging the data. That would lead to values from during the war
being lagged and those lags would not be removed when we remove the war
years.

Therefore, we are going to do the deletion of the war years in two
steps: First, we will overwrite all values corresponding to war years
with \texttt{NA}. This will ensure that no values corresponding to war
years will be lagged `outside' the war periods. Then we can lag the
data. Here again we need to make sure to treat the data per country when
lagging it over time, else we end up with data originally corresponding
to one country now corresponding to another one. To prevent this, we are
going to group the data by country and then employ the function
\emph{do()} from the package \texttt{dplyr}. It can evaluate the
function that we use to lag the variables per group, similar to the way
the function \emph{mutate()} works. Finally we completely remove the war
years from the data set.

Let us start with replacing of the observations for the war years by NA
values. As suggested by Schularick and Taylor, we will extend the time
for values being removed to 1919 and 1947. We can use the function
\emph{ifelse()} within the function \emph{mutate()} to replace the
correct values. \emph{ifelse()} will evaluate a condition that we
specify and will then evaluate one of the two cases and return the
obtained value.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#overwrite  war years with NA}
\NormalTok{data =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }
              \CommentTok{# remove values in war years in dlloansr}
              \CommentTok{# '|' is a logical operator with the meaning 'OR'}
              \DataTypeTok{dlloansr =} \KeywordTok{ifelse}\NormalTok{(year <}\StringTok{ }\DecValTok{1914} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1919}\NormalTok{, dlloansr, }\OtherTok{NA}\NormalTok{),}
              \DataTypeTok{dlloansr =} \KeywordTok{ifelse}\NormalTok{(year <}\StringTok{ }\DecValTok{1939} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1947}\NormalTok{, dlloansr, }\OtherTok{NA}\NormalTok{),}
              \CommentTok{# remove values in war years in dlnmr}
              \DataTypeTok{dlnmr =} \KeywordTok{ifelse}\NormalTok{(year <}\StringTok{ }\DecValTok{1914} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1919}\NormalTok{, dlnmr, }\OtherTok{NA}\NormalTok{),}
              \DataTypeTok{dlnmr =} \KeywordTok{ifelse}\NormalTok{(year <}\StringTok{ }\DecValTok{1939} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1947}\NormalTok{, dlnmr, }\OtherTok{NA}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Note that for World War 2, we overwrote data with NA values up to year
1947. This is suggested by Schularick and Taylor in order to eliminate
the aftermath of the war and necessary to reproduce the exact numbers
from their paper.

For the last one of the three variables, \textbf{dlmr}, go ahead and do
the calculation yourself.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{dlmr =} \KeywordTok{ifelse}\NormalTok{(year <}\StringTok{ }\DecValTok{1914} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1919}\NormalTok{, dlmr, }\OtherTok{NA}\NormalTok{),}
                    \DataTypeTok{dlmr =} \KeywordTok{ifelse}\NormalTok{(year <}\StringTok{ }\DecValTok{1939} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1947}\NormalTok{, dlmr, }\OtherTok{NA}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

You can now click the data button to see whether your changes were
correctly executed.

In the next step, you will see how the data get grouped and the
variables get lagged. Note that the function \emph{lag()} has been
modified for you to calculate all five lagged series at once. Thus you
have to call the function \emph{mylag()} instead, to which you can pass
a vector containing the different numbers of lags that you want to
calculate.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create lags - use do() to create lags for each group. }
\CommentTok{# seq(5) = c(1,2,3,4,5) --> create 5 lagged series with corresponding lags}
\NormalTok{lag.dlloansr =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(data, iso) %>%}\StringTok{ }\KeywordTok{do}\NormalTok{(., }\KeywordTok{mylag}\NormalTok{(.$dlloansr, }\KeywordTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{)))}

\CommentTok{# merge lags to data frame 'data'}
\NormalTok{data =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(data, }\KeywordTok{select}\NormalTok{(}\KeywordTok{ungroup}\NormalTok{(lag.dlloansr), -iso)) }
\end{Highlighting}
\end{Shaded}

In the second line we have merged the lags into the data frame
\textbf{data} so that we can easily pass it, together with the other
variables, to functions in later tasks.

The next code chunk serves to double check whether we did no mistake at
the transitions between different countries.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# have a look into the data set}
\KeywordTok{group_by}\NormalTok{(data, iso) %>%}\StringTok{ }\KeywordTok{do}\NormalTok{(.[}\DecValTok{3}\NormalTok{:}\DecValTok{5}\NormalTok{,]) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(iso, year, dlloansr, dlloansr.lag1, dlloansr.lag2, dlloansr.lag3) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{as.data.frame}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

{\small 
\begin{verbatim}
##    iso year     dlloansr dlloansr.lag1 dlloansr.lag2 dlloansr.lag3
## 1  AUS 1872           NA            NA            NA            NA
## 2  AUS 1873           NA            NA            NA            NA
## 3  AUS 1874  0.041861825            NA            NA            NA
## 4  CAN 1872  0.136284209   0.178761204            NA            NA

   ¦    ¦    ¦            ¦             ¦             ¦             ¦
   
## 39 SWE 1874  0.158459176   0.150170789   0.152490751            NA
## 40 USA 1872           NA            NA            NA            NA
## 41 USA 1873           NA            NA            NA            NA
## 42 USA 1874           NA            NA            NA            NA
\end{verbatim}
} 

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# display the transition between Australia and  Canada}
\KeywordTok{select}\NormalTok{(data, iso, year, dlloansr, dlloansr.lag1,}
		\NormalTok{dlloansr.lag2, dlloansr.lag3)[}\DecValTok{136}\NormalTok{:}\DecValTok{144}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##     iso year   dlloansr dlloansr.lag1 dlloansr.lag2 dlloansr.lag3
## 136 AUS 2005 0.09543597    0.09214402    0.11292995    0.13424912
## 137 AUS 2006 0.09219629    0.09543597    0.09214402    0.11292995
## 138 AUS 2007 0.15026233    0.09219629    0.09543597    0.09214402
## 139 AUS 2008 0.09013823    0.15026233    0.09219629    0.09543597
## 140 CAN 1870         NA            NA            NA            NA
## 141 CAN 1871 0.17876120            NA            NA            NA
## 142 CAN 1872 0.13628421    0.17876120            NA            NA
## 143 CAN 1873 0.10980566    0.13628421    0.17876120            NA
## 144 CAN 1874 0.20185933    0.10980566    0.13628421    0.17876120
\end{verbatim}
} 

Now it is your turn to repeat the lagging for the two variables
\textbf{dlnmr} and \textbf{dlmr}. You should save the results in
\textbf{lag.dlnmr} and \textbf{lag.dlmr}. Then merge the results into
the data frame \textbf{data}. Make sure to remove the grouping index
from your lagged data frames, as it is done above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lag.dlnmr =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(data, iso) %>%}\StringTok{ }\KeywordTok{do}\NormalTok{(., }\KeywordTok{mylag}\NormalTok{(.$dlnmr, }\KeywordTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{)))}
\NormalTok{lag.dlmr =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(data, iso) %>%}\StringTok{ }\KeywordTok{do}\NormalTok{(., }\KeywordTok{mylag}\NormalTok{(.$dlmr, }\KeywordTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{)))}
\NormalTok{data =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(data, }\KeywordTok{select}\NormalTok{(}\KeywordTok{ungroup}\NormalTok{(lag.dlnmr), -iso), }\KeywordTok{select}\NormalTok{(}\KeywordTok{ungroup}\NormalTok{(lag.dlmr), -iso))}
\end{Highlighting}
\end{Shaded}

Finally, we can completely remove the war years from the data set. You
can see how this is done for World War 1, and should do it in the same
way for World War 2 (recall that we remove the years 1939 - 1947,
following the suggestion of Schularick and Taylor).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove war years}
\NormalTok{data =}\StringTok{ }\KeywordTok{filter}\NormalTok{(data, year <}\StringTok{ }\DecValTok{1914} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1919}\NormalTok{)}
\NormalTok{data =}\StringTok{ }\KeywordTok{filter}\NormalTok{(data, year <}\StringTok{ }\DecValTok{1939} \NormalTok{|}\StringTok{ }\NormalTok{year >}\StringTok{ }\DecValTok{1947}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By the same idea (replace war values with NA, lag the variable group
wise, then drop all observations), the following two variables also got
lagged.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  log(total loans / GDP)
\item
  log(total loans / broad money)
\end{itemize}

To prevent the user from having to repeat the same task too often, these
lagged series have been added to the data set in the next part of the
exercise without their creation being explicitly shown.

\subsubsection{\sf Exercise 3.2 -- Predicting Crises with Linear Probability Models}\label{exercise-3ux5f2-probability-models-to-forecast-a-crisis}

First we need to load the data (see the previous part of this exercise
for their preparation).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{file =} \StringTok{"data_3_1.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Schularick and Taylor use their data to predict financial crises induced
by credit bubbles. For this purpose they suggest two forecasting models
which both address the question whether the credit growth data over a
five year period can be used to prognosticate a financial crisis.

To be exact, we will be working with two probability models. When we
estimate the probability of a crisis in a certain country and in a given
year, we estimate the probability for an event with only two outcomes,
crisis or no crisis. Recall from exercise three that in the data set we
are going to load we find a binary variable \textbf{crisisST} which
describes when a crisis happened in the the data. This variable will be
the dependent variable in the probability models and in the following we
will try to find whether some of the economic indicators like the credit
aggregates total bank loans and total bank assets, money aggregates, or
aggregates derived from a combination of the variables in the data set
can explain or even forecast this variable.

After specifying and estimating a model, we are going to test for its
prediction quality of its estimates with different testing techniques.

The two proposed models are a linear probability model where the
coefficients will be estimated with an ordinary least squares approach
and a logit probability model, which has the advantage that the
predicted probabilities cannot be outside the interval $[0,1]$, which
can happen (and as we will see in fact happens with our data) in the
linear model.

\infobox{Info: Linear probability models}{

As we just mentioned, we will be using a binary variable as dependent
variable. Thus if we used a linear model in the form of
\[y = b_0 + b_1 x_1 + ... + b_k x_k + e\] we could not interpret the
coefficients in the usual way, which is, holding everything else fixed a
change of one unit in for example $x_1$ would result in a change of
$b_1$ in $y$. The problem here is of course that $y$ could only take on
the values zero and one. On the other hand we can use this discrete
nature of $y$ to calculate the expectation $E(y)$:
\[E(y) = P(y = 0) \cdot 0 + P(y=1) \cdot 1 = P(y=1)\] When we can rely
on one of the classical linear model assumptions of a zero conditional
mean (cf.~Wooldridge (2012)), which is \[E(u | x_1,...,x_k) = 0,\] then
we get \[E(y | x) = P(y=1|x) = b_0 + b_1 x_1 + ... + b_k x_k.\]

Therefore we can interpret the outcome $\hat{y}$ from the linear model
as the probability $P(y=1|x)$ and this allows us to interpret the
coefficients in the usual way.

This approach is not without problems. See the ``Logit probability
models'' info block for a possible improvement.
}

\begin{infobox2}
\textbf{Info: Logit probability models}

In the linear probability model, the values for the dependent variables
lie on a straight line. If the slope of this line is not zero (as it
will be in all non-trivial cases) there will possibly be values for the
dependent variables outside the interval $[0,1]$. This is demonstrated
with random sample data in the picture that you get when you check the
next code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# x = -6,...,6}
\NormalTok{x =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \NormalTok{-}\DecValTok{6}\NormalTok{, }\DataTypeTok{to =} \DecValTok{6}\NormalTok{, }\DataTypeTok{by = }\FloatTok{0.5}\NormalTok{)}

\CommentTok{# sample binary vector}
\NormalTok{y =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}

\CommentTok{# draw plot with linear regression line}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{qplot}\NormalTok{(x,y) +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =}\StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/3_2__65-1.pdf}

Obviously, this shows that the relation between outcomes which should be
interpreted as probabilities and their corresponding inputs cannot
perfectly described by a linear regression model.

This problem is addressed by the logit probability model. With the logit
probability model we estimate the relation between the independent
variables and the quantity $\text{logit}(p) = \ln(\frac{p}{1-p})$ which
leaves us with: \[\text{logit}(p) = b_0 + b_1 x_1 + ... + b_k x_k + e\]

Since the logit function is a bijection between $[0,1]$ and the whole
real line we do not run into difficulties with the interpretation of the
dependent variables as $\text{logit}(p)$ of some probability $p$.

If we apply the inverse function $\text{logit}^{-1}(p)$ to the dependent
variable of the logit regression, we can use it as an estimate of $p$.
It is \[\text{logit}^{-1}(x) = \frac{1}{1+\exp(-x)} =: p\] and looks as
follows (check the next code chunk).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# x = -6,...,6 (smaller intervals for smooth line)}
\NormalTok{x =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \NormalTok{-}\DecValTok{6}\NormalTok{, }\DataTypeTok{to =} \DecValTok{6}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.25}\NormalTok{)}

\CommentTok{# y = 1 / (1 + exp(-x))}
\NormalTok{y =}\StringTok{ }\DecValTok{1} \NormalTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \NormalTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(-}\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \NormalTok{-}\DecValTok{6}\NormalTok{, }\DataTypeTok{to =} \DecValTok{6}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.25}\NormalTok{)))}

\CommentTok{# create line chart}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{qplot}\NormalTok{(x, y, }\DataTypeTok{geom =} \StringTok{"line"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/3_2__66-1.pdf}
\end{infobox2}
\quad

The two models are defined as follows:

Linear model:
\[p_{it} = b_{0} + b_1(L) \text{diff}(\text{log}(\text{Credit Growth}_{it})) + b_{2k} X_{itk} + e_{it}\]
Logit model:
\[\text{logit}(p_{it}) = b_{0} + b_1(L) \text{diff}(\text{log}(\text{Credit Growth}_{it})) + b_{2k} X_{itk} + e_{it}\]

As mentioned in the info block, logit($p$)$= \ln(\frac{p}{1-p})$ and $L$
is the lag operator with $b_1(L)$ being a vector which contains the
coefficients for each order of the lagged variable. We will lag
different credit aggregates (the default will be total bank loans
deflated by the CPI) to determine whether these most recent past credit
data of the preceding years can imply a crisis. We will be using lagged
variables of the order one to five (as produced in the first part of
this exercise), so this leaves us with five coefficients in $b(L)$.

It is worth noting at this point that Schularick and Taylor point out
that in many cases less lags would have been sufficient but since some
model specifications lead to significant estimates for higher order
lags, they propose to maintain five lags. $X_{itk}$ will be $k$
additional variables, with observations for each country and time. These
variables will only be present in some cases and thus specified in
detail whenever they are deployed in a model.

We will start with the estimation of an easy pooled linear model for
which we will regress lagged series of the data of total bank loans on
the binary variable. To keep things neatly arranged, we will first
select the specific variables from the data set \textbf{data} and keep
them in a new data frame \textbf{data\_reg1}. If you want to have a look
at \textbf{data\_reg1}, either use the data explorer by clicking
\texttt{data} or uncomment the line in the code to have a glance at some
of the elements.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select data}
\NormalTok{data_reg1 =}\StringTok{ }\KeywordTok{select}\NormalTok{(data, crisisST, }\KeywordTok{contains}\NormalTok{(}\StringTok{"dlloansr.lag"}\NormalTok{))}
\CommentTok{# data_reg1[4:10,]}
\CommentTok{# Uncomment the previous line of code for a glance at some of the data;}
\CommentTok{# Use the brackets like this: [which rows, which columns]. If you leave }
\CommentTok{# the field for either one blank, all lines (or columns respectively) will  }
\CommentTok{# be shown (unchanged, the code shows rows 4 - 10 for all columns)}
\end{Highlighting}
\end{Shaded}

Before we begin with the estimation of the model, we select data for a
second and third linear model. In these models we will add country fixed
effects to model (2) and country and year fixed effects to model (3)
(dummy variables for country and year; cf.~exercise 1) as additional
variables. Therefore you should create two data frames in the same way,
but also include the variables \textbf{iso} and \textbf{year}. Assign
the data frames to \textbf{data\_reg2.tmp} and \textbf{data\_reg3.tmp}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select data for regression 2}
\NormalTok{data_reg2.tmp =}\StringTok{ }\KeywordTok{select}\NormalTok{(data, crisisST, }\KeywordTok{contains}\NormalTok{(}\StringTok{"dlloansr.lag"}\NormalTok{), iso)}


\CommentTok{# select data for regression 3}
\NormalTok{data_reg3.tmp =}\StringTok{ }\KeywordTok{select}\NormalTok{(data, crisisST, }\KeywordTok{contains}\NormalTok{(}\StringTok{"dlloansr.lag"}\NormalTok{), iso, year)}
\end{Highlighting}
\end{Shaded}

Besides selecting the variables we now have to make sure the fixed
effect variables will be from class \texttt{factor} so that function
\emph{lm()} can recognise them as dummies. Thus we need to convert them.
Just skim through the comments and then check the code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# adapt the right classes}
\NormalTok{data_reg2 =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data_reg2.tmp,}
                   \CommentTok{# for country dummies in regression model}
                   \DataTypeTok{iso.dummy =} \KeywordTok{as.factor}\NormalTok{(iso),}
                   \CommentTok{# Schularick and Taylor employ also year indicators }
                   \DataTypeTok{iso =} \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(iso))}
                   \NormalTok{)}



\CommentTok{# adapt the right classes}
\NormalTok{data_reg3 =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data_reg3.tmp,}
                   \CommentTok{# as before}
                   \DataTypeTok{iso.dummy =} \KeywordTok{as.factor}\NormalTok{(iso),}
                   \DataTypeTok{iso =} \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(iso)),}
                   \CommentTok{# for year dummies in regression model}
                   \DataTypeTok{year =} \KeywordTok{as.factor}\NormalTok{(year)}
                   \NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that we selected just those variables to be saved in the respective
data frames \textbf{data\_regX}~(X~=~1,~2,~\ldots{}) that will be used
as variables in the regression. To check again which variables these are
you can use the function \emph{names()}, as done in the first line of
the following code chunk. We can then pass the data frames to the
function \emph{lm()} to get the estimates of the coefficients of these
linear probability models. We determine the variable \textbf{crisisST}
as dependent variable and all other variables within the respective data
frames as explanatory variables. This can be achieved by specifying the
dependent variable in the formula we are going to pass to \emph{lm()} on
the left side of the \texttt{\textasciitilde{}} and setting a dot
\texttt{.} on the right side, which indicates to use all other variables
here.

See how it is done for the first model and then repeat the task for the
second and third models and assign the results to \textbf{reg2} and
\textbf{reg3}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show names of variables within 'data_reg1'}
\KeywordTok{names}\NormalTok{(data_reg1)}
\end{Highlighting}
\end{Shaded}

\vskip -3em\quad
{\small
\begin{verbatim}
## [1] "crisisST"      "dlloansr.lag1" "dlloansr.lag2" "dlloansr.lag3"
## [5] "dlloansr.lag4" "dlloansr.lag5"
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# carry out regression for model specification 1}
\NormalTok{reg1 =}\StringTok{ }\KeywordTok{lm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =} \NormalTok{data_reg1) }

\CommentTok{# carry out regression for model specification 2}
\NormalTok{reg2 =}\StringTok{ }\KeywordTok{lm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =} \NormalTok{data_reg2) }

\CommentTok{# carry out regression for model specification 3}
\NormalTok{reg3 =}\StringTok{ }\KeywordTok{lm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =} \NormalTok{data_reg3) }
\end{Highlighting}
\end{Shaded}

Now we should print the regression results. We can display the results
neatly with the function \emph{showreg()} from the package
\texttt{regtools}. This function also allows us to correct for robust
standard errors if we set the two parameters `robust' and `robust.type'
appropriately (we will use the argument ``HC1'' in order to account for
the Huber/White-estimator used in the original code). Also, since we are
only interested in how well the credit aggregates of the past years (the
lagged total loans series in this model specification) work in the
estimation of our model, we will remove other coefficients from the
results with the parameter `omit.coef'. This parameter expects one
string containing all coefficients that should be removed, the
respective parameters being separated by \texttt{\textbar{}}.

To save you the typing (we want to remove all dummy parameters, roughly
150!), in the next code chunk you will just find this and further
preparation for the table with the result. The actual table will be
created from the call in the subsequent code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# here, grep() returns everything but the 'pattern' from names(coef(regX))}
\CommentTok{# names(coef(regX)) returns the names of the coefficients in the respective model}
\NormalTok{exclude =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{grep}\NormalTok{(}\DataTypeTok{pattern =} \StringTok{"dlloansr.lag"}\NormalTok{, }\KeywordTok{names}\NormalTok{(}\KeywordTok{coef}\NormalTok{(reg2)), }
                 \DataTypeTok{value =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{invert =} \OtherTok{TRUE}\NormalTok{),}
            \KeywordTok{grep}\NormalTok{(}\DataTypeTok{pattern =} \StringTok{"dlloansr.lag"}\NormalTok{, }\KeywordTok{names}\NormalTok{(}\KeywordTok{coef}\NormalTok{(reg3)), }
                 \DataTypeTok{value =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{invert =} \OtherTok{TRUE}\NormalTok{))}

\CommentTok{# so far, we have a vector of strings. 'omit.doef' expects one string, so we}
\CommentTok{# collapse the strings into one string separated by '|'}
\NormalTok{exclude_OLS =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(exclude, }\DataTypeTok{collapse =} \StringTok{"|"}\NormalTok{)}

\CommentTok{# feel free to uncomment the next line to see which actual coefs get excluded}
\CommentTok{# exclude_OLS}

\CommentTok{# adapt model names}
\NormalTok{modelnames_OLS =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"OLS None (1)"}\NormalTok{, }\StringTok{"OLS Country (2)"}\NormalTok{, }\StringTok{"OLS Country + Year (3)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\vskip -.5em \COMMENT{The numbering of the model specifications has been done in accordance with the numbering in the original paper to make comparing easier.}
\vskip 0em \begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show regession results with showreg() from regtools package}
\KeywordTok{library}\NormalTok{(regtools)}

\CommentTok{# show results neatly with showreg() }
\CommentTok{# additional arguments account for robust standard errors, the model names,}
\CommentTok{# how many digits are displayed, whether some statistics should be }
\CommentTok{# displayed, and which coefficients should not be shown}
\KeywordTok{showreg}\NormalTok{(}\KeywordTok{list}\NormalTok{(reg1, reg2, reg3), }\DataTypeTok{robust =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{robust.type =} \StringTok{"HC1"}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{, }
        \DataTypeTok{omit.coef =} \NormalTok{exclude_OLS, }\DataTypeTok{custom.model.names =} \NormalTok{modelnames_OLS, }
        \DataTypeTok{include.adjrs =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{include.deviance =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{include.fstatistic = }\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\vskip 0.5em \begin{SAMEPAGE}
{\small
\begin{verbatim}
## ====================================================================
##                OLS None (1)  OLS Country (2)  OLS Country + Year (3)
## --------------------------------------------------------------------
## dlloansr.lag1    -0.0281       -0.0273          -0.0489             
##                  (0.1050)      (0.1055)         (0.0958)            
## dlloansr.lag2     0.3012 *      0.3021 *         0.3195 **          
##                  (0.1186)      (0.1196)         (0.1053)            
## dlloansr.lag3     0.0486        0.0478           0.0013             
##                  (0.1146)      (0.1142)         (0.1064)            
## dlloansr.lag4     0.0049        0.0021           0.0346             
##                  (0.0601)      (0.0610)         (0.0682)            
## dlloansr.lag5     0.0979        0.0928           0.1361             
##                  (0.0774)      (0.0771)         (0.0806)            
## --------------------------------------------------------------------
## R^2               0.0158        0.0230           0.2958             
## Num. obs.           1272          1272             1272                  
## F statistic       4.0609        1.6382           3.8515             
## ====================================================================
## *** p < 0.001, ** p < 0.01, * p < 0.05
\end{verbatim}
}
\end{SAMEPAGE}
\quad

Always the second coefficient in these models statistically differs
significantly from zero. As for the interpretation of these coefficients
we quote Schularick and Taylor verbally (p.~1044): ``In all of the OLS
models the sum of the lag coefficients is about 0.40, which is easy to
interpret. Average real loan growth over five years in this sample has a
standard deviation of about 0.07, so a one standard deviation change in
real loan growth increases the probability of a crisis by about 0.0280,
or 2.8 percentage points. Since the sample frequency of crises is just
under four percent, this shows a high sensitivity of crises to plausible
shocks within the empirical range of observed loan growth
disturbances.''

We can use an Added-Variable (AV) plot to examine the relation between
the coefficients and the dependent variable in more detail. This kind of
a plot is similar to a scatter plot of a regression model of only one
independent variable $x$ against $y$ and allows us to display the
influence of each explanatory variable on the model.

\vskip 2em \infobox{Info: Added-Variable plot}{

An Added-Variable plot shows the residuals from the dependent variable
against all independent variables versus the
residuals from the `added variable' against the other independent
variables. For more info, see for example Chatterjee and Hadi (2015).
}
\quad

Since the coefficients are all similar, we will use the easiest model
(\textbf{reg1}) to demonstrate the AV plot. For the other two models we
would have to exclude all the dummies again, which is a bit stressful
and does not add much valuable information.

Added variable plots can be produced in R with the function
\emph{avPlots()} from the package \texttt{car}. This function is easy to
use, you can simply pass the object containing the model (\textbf{reg1})
to it.

\vskip 2em \begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load car package}
\KeywordTok{library}\NormalTok{(car)}

\CommentTok{# now create the avPlot}
\CommentTok{# construct partial-regression plots for 'reg1'}
\KeywordTok{avPlots}\NormalTok{(reg1) }
\end{Highlighting}
\end{Shaded}

\includegraphics{creditboomsgonebust_output_solution_files/figure-latex/3_2__73-1.pdf}

In the figure you can see the resulting added variable plots for each
explanatory variable within the model. Also fitted is the regression
line in red, corresponding to the respective coefficients. Due to the
nature of \textbf{crisisST} being a binary variable being zero most of
the time (crises are naturally rare events) most points are found around
$y = 0$ and only some around $y = 1$ (the deviation from the exact
values zero and one comes from the fact that we are plotting the
residuals). The resulting two separate, parallel sequences of data
points which we use for the fitting reflect the background of
probability models as described in the info block above.

The relatively flat slopes here are not surprising because that leads to
the predictions (the fitted values) from the model being rather small.
Recall that the prediction value is interpreted as the likelihood for a
crisis at that point, and this value should be rather low if we think of
the rarity of crises again.

However, the intercept from the model (which we haven't reported to keep
things close to the original results from Schularick and Taylor; it is
0.02) is very small, which means the coefficient for the credit growth
`2 years ago' (total loans lagged twice) with 0.3 runs more or less
through the origin.

\textbf{Question:} What is the risk of the described matter?

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{A:} There is no risk
\item
  \textbf{B:} Only one coefficient is significant
\item
  \textbf{C:} The probability predictions might be negative
\end{itemize}

Assign your answer to \textbf{Answer} as a string (uncomment it first).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Answer}
\NormalTok{Answer =}\StringTok{ "C"}
\end{Highlighting}
\end{Shaded}

\begin{infobox2}
\textbf{Info: Answer}

Answer C is correct, we are most likely left with negative predicted
probabilities as described in the info blocks about probability models.
Let us check whether this is the case here. How to check is shown for
\textbf{reg1}, do it yourself for \textbf{reg2} and \textbf{reg3}.

\quad
\begin{minipage}{\textwidth}
\setlength{\parskip}{-1cm}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sum of negative fitted values from model 1}
\KeywordTok{sum}\NormalTok{(reg1$fitted.values <}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 53
\end{verbatim}
}
\vskip -8pt \quad 
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(reg2$fitted.values <}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 95
\end{verbatim}
}
\vskip -8pt \quad 
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(reg3$fitted.values <}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 462
\end{verbatim}
}
\end{minipage}
\quad

You can see that our assumption was correct. Model three, the one
including dummy countries and years has the worst performance in this
respect. Therefore, in the next part of the exercise we are going to use
the proposed logit model instead.
\end{infobox2}

\subsubsection{\sf Exercise 3.3 -- Predicting Crises with Logit Probability Models}\label{exercise-3ux5f3-logit-probability-models-to-forecast-a-crisis}

This exercise extends the results from exercise 3.2, thus we will start
with reading data from the preceding exercise.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in data}
\KeywordTok{load}\NormalTok{(}\StringTok{"data_3_3.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As we saw in exercise 3.2, we run into some problems with the linear
probability model, which is why we switch to using the logit probability
model in this part of the exercise.

\[\text{logit}(p_{it}) = b_{0} + b_1(L) \text{diff}(\text{log}(\text{Credit Growth}_{it})) + b_{2k} X_{itk} + e_{it}\]

with logit($p$)$= \ln(\frac{p}{1-p})$ and everything else like in the
linear case. Thus we will again be looking at whether recent past credit
data will be able to forecast a crisis.

We start with setting up data for four different logit models. The first
and second one will be using the same variables that we were using for
the models (1) and (2), which means in one case (model (4)) we will
regress \textbf{crisisST} on lagged total loans data, and in the other
one (model (5)) we add country fixed effects to the explanatory
variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up data for regression}
\NormalTok{data_lreg4 =}\StringTok{ }\NormalTok{data_reg1}
\NormalTok{data_lreg5 =}\StringTok{ }\NormalTok{data_reg2}
\end{Highlighting}
\end{Shaded}

To keep things easily comparable, note that in the paper by Schularick
and Taylor model (5) and (6) are the very same ones, just presented in
different tables. We skip (6) and continue with model (7). In model (7)
we are going to use lagged data of broad money growth
(\textbf{dlmr.lagX} with $X = 1,...,5$) instead of lagged data of bank
loans, to see how in comparison recent past data of money can imply a
crisis, and in model (8) we use lagged data of narrow money growth
(\textbf{dlnmr.lagX} with $X =1,...,5$) instead. Recall that as well as
total loans, both money growth aggregates got deflated with the consumer
price index when we set up the lags in exercise 3.1. To replace the
lagged data of loans, we remove these variables in a first step from
model (5). Then we merge the remaining variables with the lagged money
aggregates.

The data for model (7) has been set up for you, but you should set up
the data for model (8) yourself. Assign it to \textbf{data\_lreg8}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove lagged loan variables from data for model 5}
\NormalTok{data_lreg5_woL =}\StringTok{ }\KeywordTok{select}\NormalTok{(data_lreg5, -}\KeywordTok{contains}\NormalTok{(}\StringTok{"dlloansr.lag"}\NormalTok{))}

\CommentTok{# add lagged broad money variables instead}
\NormalTok{data_lreg7 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, }\KeywordTok{contains}\NormalTok{(}\StringTok{"dlmr.lag"}\NormalTok{)), data_lreg5_woL)}

\CommentTok{# add lagged narrow money variables instead}
\NormalTok{data_lreg8 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, }\KeywordTok{contains}\NormalTok{(}\StringTok{"dlnmr.lag"}\NormalTok{)), data_lreg5_woL)}
\end{Highlighting}
\end{Shaded}

The computation of logit regression models in R is very similar to the
computation of linear models. The biggest difference is that we use the
function \emph{glm()} instead of \emph{lm()}, which works for a number
of different generalized linear models. In our case, we need to bind the
parameter `family' to the string ``binomial'' to compute a logit model.

The code has been prepared for models (4), (5), and (7). Adapt the code
for model (8) and assign it to lreg8.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the model estimates}
\NormalTok{lreg4 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg4, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }
\NormalTok{lreg5 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg5, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }
\NormalTok{lreg7 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg7, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }
\NormalTok{lreg8 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg8, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Next we use the function \emph{showreg()} from the package
\texttt{regtools} again to display the results. As before, you will have
to check the short code chunk first, in which the model names to use and
which coefficients to omit will be prepared. Then you can run the
subsequent code chunk to present the regression results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# here, grep() returns everything but the 'pattern' from names(coef(lregX))}
\CommentTok{# names(coef(lregX)) returns the names of the coefficients in the respective model}
\NormalTok{exclude =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{grep}\NormalTok{(}\DataTypeTok{pattern =} \StringTok{"dlloansr.lag"}\NormalTok{, }\KeywordTok{names}\NormalTok{(}\KeywordTok{coef}\NormalTok{(lreg5)), }
                 \DataTypeTok{value =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{invert =} \OtherTok{TRUE}\NormalTok{))}

\CommentTok{# so far, we have a vector of strings. 'omit.coef' expects one string, so we}
\CommentTok{# collapse the strings into one string separated by '|'}
\NormalTok{exclude_lgt =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(exclude, }\DataTypeTok{collapse =} \StringTok{"|"}\NormalTok{)}

\CommentTok{# feel free to uncomment the next line to see which actual coefs get excluded}
\CommentTok{# exclude_logit}

\CommentTok{# adapt model names}
\NormalTok{modelnames_lgt =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Logit None (4)"}\NormalTok{, }\StringTok{"Baseline (5)"}\NormalTok{, }
                   \StringTok{"Broad Money (7)"}\NormalTok{, }\StringTok{"Narrow Money (8)"}\NormalTok{)}
\CommentTok{# show regression results}
\KeywordTok{showreg}\NormalTok{(}\KeywordTok{list}\NormalTok{(lreg4, lreg5, lreg7, lreg8), }\DataTypeTok{robust =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{robust.type =} \StringTok{"HC4m"}\NormalTok{, }
\DataTypeTok{digits =} \DecValTok{3}\NormalTok{, } \DataTypeTok{omit.coef =} \NormalTok{exclude_lgt, }\DataTypeTok{custom.model.names =} \NormalTok{modelnames_lgt, }
        \DataTypeTok{include.aic =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{include.bic = }\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{include.deviance =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\vskip 1em \begin{SAMEPAGE}
\linespread{0.95}
{\small
\begin{verbatim}
## ===============================================================================
##                 Logit None (4)  Baseline (5)  Broad Money (7)  Narrow Money (8)
## -------------------------------------------------------------------------------
## dlloansr.lag1     -0.257          -0.398                                       
##                   (2.223)         (2.340)                                      
## dlloansr.lag2      6.956 **        7.138 *                                     
##                   (2.436)         (2.817)                                      
## dlloansr.lag3      1.079           0.888                                       
##                   (2.934)         (3.157)                                      
## dlloansr.lag4      0.290           0.203                                       
##                   (1.362)         (1.525)                                      
## dlloansr.lag5      2.035           1.867                                       
##                   (1.676)         (1.740)                                      
## dlmr.lag1                                       -1.051                         
##                                                 (2.875)                        
## dlmr.lag2                                        5.773 *                       
##                                                 (2.764)                        
## dlmr.lag3                                        3.515                         
##                                                 (2.550)                        
## dlmr.lag4                                       -1.535                         
##                                                 (2.455)                        
## dlmr.lag5                                        3.077                         
##                                                 (2.352)                        
## dlnmr.lag1                                                       -2.504        
##                                                                  (1.878)       
## dlnmr.lag2                                                        2.303        
##                                                                  (2.083)       
## dlnmr.lag3                                                        1.768        
##                                                                  (1.866)       
## dlnmr.lag4                                                       -2.880        
##                                                                  (1.595)       
## dlnmr.lag5                                                        1.373        
##                                                                  (1.678)       
## -------------------------------------------------------------------------------
## Log Likelihood  -210.760        -205.792      -224.591         -237.440        
## Num. obs.           1272            1272          1348             1381            
## ===============================================================================
## *** p < 0.001, ** p < 0.01, * p < 0.05
\end{verbatim}
}
\end{SAMEPAGE}

We can see that apart from model (8), the model in which we used lagged
variables of deflated narrow money growth instead of loan growth is the
only one here which does not give a statistically significant
coefficient. Model (7) (broad money growth) on the other hand shows a
significant coefficient and Schularick and Taylor state that it could be
used as an alternative for credit growth. Nevertheless, we can deduce
from the log likelihood estimates presented in the table that the fit of
broad money is a bit weaker than those fits with credit aggregates
(model (4) and (5)).

Comparing the latter two models based on the recent past credit growth,
we see that both show statistically significant coefficients for the
second lag. To see whether the added country fixed effects (whose
estimated coefficients we have excluded from the displayed results in
accordance with Schularick and Taylor) improve the regression model, we
can employ a Wald test.
\\

\infobox{Info: Wald test}{

To see whether there is statistical evidence of a coefficient
contributing significantly to a model, a Wald test can be carried out.
In a Wald test, maximum likelihood estimates of one or more coefficients
get compared to their standard errors (remember that we use robust
standard errors for our estimation). The Wald test then tests against
the null hypothesis that all coefficients being tested are equal to zero
(Lancelot \& Lesnoff, 2012). For more info, see for example Hosmer and
Lemeshow (2004).
}


The package \texttt{aod} by Lancelot \& Lesnoff (2012) contains a
function \emph{wald.test()}. This function lets us carry out the Wald
test. We have to specify the coefficients that should get tested with
the argument `Terms' and all coefficients in general with the argument
`b'. As you will see when you run the code chunk, one coefficient is NA.
It was not estimated due to collinearity. When we pass the coefficients
to `b' we have to remove this NA value, else function \emph{wald.test()}
will not work. Like with the displaying of the results, we also correct
for robust standard errors by passing a modified covariance matrix to
the parameter `Sigma'.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# just display all coefficients}
\KeywordTok{coef}\NormalTok{(lreg5)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##   (Intercept) dlloansr.lag1 dlloansr.lag2 dlloansr.lag3 dlloansr.lag4 
##   -4.57167474   -0.39838264    7.13754946    0.88839408    0.20266447 
## dlloansr.lag5           iso  iso.dummyCAN  iso.dummyCHE  iso.dummyDEU 
##    1.86658429    0.08048132   -0.91519589    0.30215696    0.62837668 
##  iso.dummyDNK  iso.dummyESP  iso.dummyFRA  iso.dummyGBR  iso.dummyITA 
##    0.49908743    0.28456574   -0.14147438    0.46049036    0.51013263 
##  iso.dummyJPN  iso.dummyNLD  iso.dummyNOR  iso.dummySWE  iso.dummyUSA 
##    0.32197766   -0.72365391   -0.49515438   -0.02923489            NA
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package 'aod'}
\KeywordTok{library}\NormalTok{(aod)}

\CommentTok{# test for all lags = 0 (equivalent to Statas testparm)}
\KeywordTok{wald.test}\NormalTok{(}\DataTypeTok{b =} \KeywordTok{coef}\NormalTok{(lreg5)[-}\DecValTok{20}\NormalTok{], }\DataTypeTok{Sigma =} \KeywordTok{vcovHC}\NormalTok{(lreg5, }\StringTok{"HC1"}\NormalTok{), }\DataTypeTok{Terms =} \DecValTok{2}\NormalTok{:}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Wald test:
## ----------
## 
## Chi-squared test:
## X2 = 17.0, df = 5, P(> X2) = 0.0045
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test for all country effects  = 0}
\KeywordTok{wald.test}\NormalTok{(}\DataTypeTok{b =} \KeywordTok{coef}\NormalTok{(lreg5)[-}\DecValTok{20}\NormalTok{], }\DataTypeTok{Sigma =} \KeywordTok{vcovHC}\NormalTok{(lreg5, }\StringTok{"HC1"}\NormalTok{), }\DataTypeTok{Terms =} \DecValTok{7}\NormalTok{:}\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Wald test:
## ----------
## 
## Chi-squared test:
## X2 = 7.6, df = 13, P(> X2) = 0.87
\end{verbatim}
}

The first test results in a p-value of 0.0045. This means that there is
statistical evidence that the lag coefficients differ from zero.
However, for the country effects we get a high p-value which is not
significant, thus we cannot reject the null hypothesis of the Wald test
and we deduce that these coefficients are not statistically
significant.\\Nevertheless, Schularick and Taylor state that they adopt
``the Logit model with country effects but without time-effects as
{[}their{]} preferred baseline specification henceforth'' (p.~1045).

Although they do not give any explanations for this decision, it is
likely that they base it on the slightly higher log likelihood estimate.
The decision is also supported by the result of a test for the
predictive power. To test for a models predictive power, Schularick and
Taylor propose in the following part to compute so called Receiver
Operating Characteristic (ROC) curves.

ROC curves are implemented in the package \texttt{pROC} and can be
calculated by the function \emph{roc()}. Let us have a look at the ROC
curve for the baseline model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package 'pROC'}
\KeywordTok{library}\NormalTok{(}\StringTok{"pROC"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Type 'citation("pROC")' for a citation.
## 
## Attaching package: 'pROC'
## 
## The following objects are masked from 'package:stats':
## 
##     cov, smooth, var
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#calculate Receiver Operating Characteristics}
\NormalTok{lreg5.ROC =}\StringTok{ }\KeywordTok{roc}\NormalTok{(}
  \CommentTok{# extract crises used in model }
  \DataTypeTok{response =} \NormalTok{lreg5$model$crisisST, }
  \CommentTok{# extract fitted values from model}
  \DataTypeTok{predictor =} \NormalTok{lreg5$fitted.values, }
  \CommentTok{# adapt additional parameters}
  \DataTypeTok{auc =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ci =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# plot ROC curve}
\KeywordTok{plot}\NormalTok{(lreg5.ROC, }\DataTypeTok{print.auc =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\IN{-2} \includegraphics{creditboomsgonebust_output_solution_files/figure-latex/3_3__83-1.pdf}

{\small
\begin{verbatim}
## Call:
## roc.default(response = lreg5$model$crisisST, 
##			   predictor = lreg5$fitted.values,
##			   auc = TRUE, ci = TRUE)
## 
## Data: lreg5$fitted.values in 1219 controls 
## (lreg5$model$crisisST 0) < 53 cases (lreg5$model$crisisST 1).
##
## Area under the curve: 0.7174
## 95% CI: 0.6489-0.7859 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute standard error}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{var}\NormalTok{(lreg5.ROC))}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 0.03494766
\end{verbatim}
}

Under the curve within the plot you see `AUC: 0.717'. AUC is short for
``area under the curve'' (also called AUROC -- area under the receiver
operating characteristic curve). In short we can say that the higher the
area under the curve, the better the predictive abilities of a model.
Note that the lowest value for the AUC you can get is 0.5 (the curve
would be equal to the diagonal line) and the highest value is 1 (the
curve would remain at 1 throughout the graph). From the 95\%-confidence
interval which is displayed in the brackets and which does not include
the lowest value 0.5 we can deduce, that the result is statistically
significant. To determine whether this AUC is a good value, let us cite
Schularick and Taylor one more time: ``Is 0.7 a `high' AUROC? For
comparison, in the medical field where ROCs are widely used for binary
classification, an informal survey of newly published prostate cancer
diagnostic tests finds AUROCs of about 0.75'' (p.~1046, footnote). For
more info on the ROC, see the info block.
\\

\infobox{Info: Receiver Operating Characteristic Curves}{

The ROC curves produced by the package \texttt{pROC} show a plot with
two axes, the x-axis showing specificity and the y-axis sensitivity.
Sensitivity and specificity are terms used in statistical testing, the
former is defined as the proportion of correctly identified positives by
a test and the latter as the proportion of correctly identified
negatives, or briefly, true positives and true negatives (Altman \&
Bland, 1994). This means that the higher both these values are, the
better is our test.

Transferred into our situation this means that sensitivity and
specificity are the rates of how often a crisis is predicted correctly
and how often it is predicted correctly that no crisis will happen. (Of
course we can use the complementary probabilities instead, too. In fact,
the x-axis is often shown as 1 - specificity.)

So far however we do not have correct or incorrect predictions of crises
yet, but only probabilities for a crisis at each respective point in the
data. Within the range of probabilities that we get from the model, we
can select an arbitrary threshold. If a predicted probability now lies
above this threshold we assume a crisis is predicted, below the
threshold no crisis is predicted. This conversion allows us to calculate
the sensitivity and specificity, but for only one threshold. If we vary
the threshold, the calculated sensitivity and specificity change, too.
This is the key point to the ROC curves. The ROC curves simply show
sensitivity and specificity as a function of the threshold. For a
further discussion of ROC curves, see Gönen (2007).
}

We should now calculate the AUC for model specification (4) (saved in
\textbf{lreg4}). As with model (5), we have to extract `response' and
`predictor' from the model. This time we are not going to plot the curve
afterwards, therefore you can just call the function \emph{roc()} and
adapt the parameters but do not have to assign the function's return
value to a name.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{roc}\NormalTok{(}
  \CommentTok{# extract crises used in model }
  \DataTypeTok{response =} \NormalTok{lreg4$model$crisisST, }
  \CommentTok{# extract fitted values from model}
  \DataTypeTok{predictor =} \NormalTok{lreg4$fitted.values, }
  \CommentTok{# adapt additional parameters}
  \DataTypeTok{auc =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ci =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Call:
## roc.default(response = lreg4$model$crisisST, 
##			   predictor = lreg4$fitted.values, 
## 			   auc = TRUE, ci = TRUE)
##
## Data: lreg4$fitted.values in 1219 controls 
## (lreg4$model$crisisST 0) < 53 cases (lreg4$model$crisisST 1).
## 
## Area under the curve: 0.6727
## 95% CI: 0.6022-0.7432 (DeLong)
\end{verbatim}
}

Do you see that the AUC is 0.673? Therefore it is smaller than the one
from model (5), which we saw was 0.717. So judging from these values,
model (5), the one specified with added country fixed effects, seems to
show a slightly better predictive power (although of course this
conclusion is not statistically proven, as you can see from the fact
that the respective AUC values are within the others respective 95\%
confidence interval).

\subsubsection{\sf Exercise 3.4 -- In- and Out-of-Sample Testing}\label{exercise-3ux5f4-further-tests-of-the-baseline-model}

We just saw that model specification (5), the one chosen as baseline
model for further testing, achieved an AUC value of 0.717 for the
in-sample prediction of a crisis. With in-sample prediction it is meant
that we tested whether the model was able to predict those crisis years
(and non crisis years) we used as the dependent variable in the
estimation of the model.

Now we want to test and compare the out-of-sample predictive ability of
the same model specification (crisis against lagged total bank loans and
country dummies), which means that we reduce the sample size for the
model estimation and use only observations from up to one year before
the year for which we will predict whether a crisis happened or not.

Check the next code chunk to read in the data needed for this part of
the exercise.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data for exercise 3.4}
\KeywordTok{load}\NormalTok{(}\StringTok{"data_3_4.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In accordance with Schularick and Taylor, we limit the prediction data
to the time after 1983. Before we start with the computation of the
out-of-sample predictions, we also limit the in-sample prediction to the
same time frame, which allows us to compare the results afterwards.
Therefore we start with computing the limited in-sample predictions. Pay
attention to the argument `newdata' that you already saw if you did
exercise one. If we use this argument it allows us to specify predictors
other than the ones used to estimate the model. Obviously it will be
necessary to specify this argument for the later out-of-sample
prediction, but we already need it now. The reason we already need it is
because else NA-values, which were dropped in the estimation of model
(5), would be dropped here, too. That would change the number of rows in
the predicted data to less rows than we need in order to be able to
merge it to the data frame \textbf{data} which contains the year values.
This however is necessary to limit the predicted values to those after
1983.

In the next chunk, the prediction and the merging has been prepared for
you, but the last part of the input in which we want to filter out
values from before 1984 you should fill in yourself. Go ahead and use
the function \emph{mutate()} to replace values within \textbf{lreg5\_in}
from before 1984 by NA values. This can be achieved by using the
function \emph{ifelse()} within \emph{mutate()}, just as it has been
done in previous exercises. If you are unsure, use the \texttt{Hint}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save in-sample predicitions, including the NA values}
\NormalTok{lreg5_in =}\StringTok{ }\KeywordTok{predict}\NormalTok{(}\DataTypeTok{object =} \NormalTok{lreg5, }\DataTypeTok{newdata =} \NormalTok{data_lreg5) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# merge to data, which contains the years}
\NormalTok{data =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(data, lreg5_in)}

\CommentTok{# now uncomment and finish the next line to replace data from before 1984 with NA}
\KeywordTok{library}\NormalTok{(dplyr)}

\CommentTok{# data = mutate(data, lreg5_in = ? )}
\NormalTok{data =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{lreg5_in =} \KeywordTok{ifelse}\NormalTok{(year <}\StringTok{ }\DecValTok{1984}\NormalTok{, }\OtherTok{NA}\NormalTok{, lreg5_in))}
\end{Highlighting}
\end{Shaded}

The out of sample prediction is somewhat more difficult: First, we
create a variable \textbf{lreg5\_out}, whose values we set to NA first.
Then we will run a loop with the years 1984 to 2008 as values of the
control variable. In each iteration we will only use data from up to but
not including the year which is the value of the control variable. We
use this restrained data to compute the same kind of logit regression as
before for each iteration. Finally we use the entire data again to
compute the predicted values, although we will only save those values
that correspond to the year of the control variable. Thus in each
iteration we will save one more set of values corresponding to one year
for each country to \textbf{lreg5\_out}.

By this method we obtain the out-of-sample predictor based on all
respective preceding data for each year between 1984 and 2008 . For
example, the predictor for 2000 is thus based on the data from 1870 to
1999.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up variable 'lreg5_out'}
\NormalTok{data =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{lreg5_out =} \OtherTok{NA}\NormalTok{)}
\NormalTok{data_lreg5y =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, year), data_lreg5)}

\CommentTok{# calculate out-of-sample predictions}
\NormalTok{for (yr in }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1984}\NormalTok{, }\DataTypeTok{to =} \DecValTok{2008}\NormalTok{))\{}
  \CommentTok{# use values from up to the year in the control variable}
  \NormalTok{regdata_f =}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_lreg5y, year <}\StringTok{ }\NormalTok{yr)}
  
  \CommentTok{# logit regression ([-1] removes year column which is not needed)}
  \NormalTok{lreg_f =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =} \NormalTok{regdata_f[-}\DecValTok{1}\NormalTok{], }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
  
  \CommentTok{# predict values, remove year and crisisST}
  \NormalTok{temp =}\StringTok{ }\KeywordTok{predict}\NormalTok{(lreg_f, data_lreg5y )}
  
  \CommentTok{# write prediction for each year in control variable into lreg5_out}
  \NormalTok{data =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(data, }\DataTypeTok{lreg5_out =} \KeywordTok{ifelse}\NormalTok{(year ==}\StringTok{ }\NormalTok{yr, temp, lreg5_out))}
  
  \CommentTok{# remove the unneeded other values}
  \KeywordTok{rm}\NormalTok{(temp)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}


To test and compare the predictions we can use ROC curves again, like we
did in the preceding part of the exercise. The next code chunk serves as
example in which we compute the in-sample AUC.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute roc - automatically drops values where either }
\CommentTok{# predictor or response is NA}
\NormalTok{insample.ROC =}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =} \NormalTok{data$crisisST,}
                   \DataTypeTok{predictor =} \NormalTok{data$lreg5_in,}
                   \DataTypeTok{auc =} \NormalTok{T, } \DataTypeTok{ci =} \NormalTok{T)}
\CommentTok{# show results}
\NormalTok{insample.ROC}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Call:
## roc.default(response = data$crisisST, 
##			   predictor = data$lreg5_in,
##			   auc = T, ci = T)
## 
## Data: data$lreg5_in in 332 controls 
## (data$crisisST 0) < 18 cases (data$crisisST 1).
##
## Area under the curve: 0.7626
## 95% CI: 0.6381-0.887 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show standard error}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{var}\NormalTok{(insample.ROC))}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 0.06349989
\end{verbatim}
}

We see that with a value of 0.763 the AUC for the limited in-sample
predictions is higher than the one from the unrestricted model, but the
standard error is slightly higher with 6.3\% here compared to 3.5\%
before. Nevertheless we can deduce that model (5) is, even when we limit
it to recent years, able to indicate crisis events. Now let us see how
well the model would have worked if it had to actually forecast a crisis
with out-of-sample predictions.

Get the ROC-results for the out-of-sample prediction yourself and assign
them to \textbf{outofsample.ROC}, then enter \textbf{outofsample.ROC}
one more time to see the results and finally compute the standard error.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute roc - automatically drops values where either }
\CommentTok{# predictor or response is NA}
\NormalTok{outofsample.ROC =}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =} \NormalTok{data$crisisST,}
                   \DataTypeTok{predictor =} \NormalTok{data$lreg5_out,}
                   \DataTypeTok{auc =} \NormalTok{T, }\DataTypeTok{ci =} \NormalTok{T)}
\NormalTok{outofsample.ROC}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Call:
## roc.default(response = data$crisisST, 
## 			   predictor = data$lreg5_out, 
##			   auc = T, ci = T)
## 
## Data: data$lreg5_out in 332 controls 
## (data$crisisST 0) < 18 cases (data$crisisST 1).
##
## Area under the curve: 0.6456
## 95% CI: 0.5093-0.7819 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show standard error}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{var}\NormalTok{(outofsample.ROC))}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 0.06953752
\end{verbatim}
}

The area under the curve is 0.646 (with a standard error of 7\%), which
means the model was not able to predict crises as well as when it had
information about future crises as in the in-sample prediction case.
This is obviously not very surprising. Therefore we should note that,
since the area under the curve is 0.646 and the 95\% confidence interval
does not include 0.5 (recall that a model with an AUC of 0.5 refers to a
model with no prediction ability), the results are statistically
significant to the 5\%-level. This means that this model would actually
have been able to predicts recent crises at least to some extent.

\subsubsection{\sf Exercise 3.5 -- Further Tests}\label{exercise-3ux5f5-pre-world-war-2-against-post-world-war-2-samples}

So far we have neglected what we showed in exercise 1, namely that there
are signals for a changed behaviour of credit growth in the time after
the Second World War, at least in relation to other economic indicators
as broad money. Nevertheless, we saw in exercise 3.3 that broad money
growth (as specified in model (7)) could substitute total loan growth
for predicting crises, even when we don't take into account the
different dynamics of these variables within each respective era.

Thus we are going to test now whether we can further improve the
predictive abilities of the models if we take the different eras into
account.

In exercise 3.3 we extracted the data used for the estimation of the
respective models (5) and (7) from the prepared data set (which can now
again be found again in \textbf{data}) and saved it in
\textbf{data\_lreg5} and \textbf{data\_lreg7}. To demonstrate that we
use the same data as before and just split both objects in two parts, we
first load these objects again and then divide the respective data into
data sets for each era.

Check the next code chunk to read in these three objects.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data for exercise 3_5}
\KeywordTok{load}\NormalTok{(}\StringTok{"data_3_5.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To be able to split each one of the two data sets (for models (5) and
(7)) into two respective data sets, we have to merge the year column to
the two model data sets first, can then filter the data and finally
remove the year column again so that we can use the prepared data for
the fitting. In the code chunk you see as an example how to split the
data for the lagged loans growth model \textbf{data\_lreg5}. Adapt the
code to set up \textbf{data\_lreg7.preWW2} and
\textbf{data\_lreg7.postWW2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{# merge years to reg data}
\NormalTok{data_lreg5y =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, year), data_lreg5)}

\CommentTok{# prepare preWW2 data for 'baseline' model (filter data; remove variable 'year')}
\NormalTok{data_lreg5.preWW2y =}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_lreg5y, year <=}\StringTok{ }\DecValTok{1945}\NormalTok{)}
\NormalTok{data_lreg5.preWW2 =}\StringTok{ }\KeywordTok{select}\NormalTok{(data_lreg5.preWW2y, -year)}

\CommentTok{# prepare postWW2 data for 'baseline' model}
\NormalTok{data_lreg5.postWW2y =}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_lreg5y, year >}\StringTok{ }\DecValTok{1945}\NormalTok{)}
\NormalTok{data_lreg5.postWW2 =}\StringTok{ }\KeywordTok{select}\NormalTok{(data_lreg5.postWW2y, -year)}
\CommentTok{# uncomment and complete the following lines by adapting the code above}
\CommentTok{# merge years to reg data}
\CommentTok{# data_lreg7y = }
\CommentTok{# }
\CommentTok{# # prepare preWW2 data for 'money growth' model}
\CommentTok{# data_lreg7.preWW2y = }
\CommentTok{# data_lreg7.preWW2 = }
\CommentTok{# }
\CommentTok{# # prepare postWW2 data for 'money growth' model}
\CommentTok{# data_lreg7.postWW2y = }
\CommentTok{# data_lreg7.postWW2 = }

\CommentTok{# merge years to reg data}
\NormalTok{data_lreg7y =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{select}\NormalTok{(data, year), data_lreg7)}

\CommentTok{# prepare preWW2 data for 'money growth' model}
\NormalTok{data_lreg7.preWW2y =}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_lreg7y, year <=}\StringTok{ }\DecValTok{1945}\NormalTok{)}
\NormalTok{data_lreg7.preWW2 =}\StringTok{ }\KeywordTok{select}\NormalTok{(data_lreg7.preWW2y, -year)}

\CommentTok{# prepare postWW2 data for 'money growth' model}
\NormalTok{data_lreg7.postWW2y =}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_lreg7y, year >}\StringTok{ }\DecValTok{1945}\NormalTok{)}
\NormalTok{data_lreg7.postWW2 =}\StringTok{ }\KeywordTok{select}\NormalTok{(data_lreg7.postWW2y, -year)}
\end{Highlighting}
\end{Shaded}

Now we use the function \emph{glm()} as we did in the previous parts to
get the regression results. Having done this we use the function
\emph{showreg()} one more time to display the results. Since this was
done often before now, you just have to check the next code blocks. For
reasons of clarity the code is splitted into several code chunks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate the logit models for each era}
\NormalTok{lreg11 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg5.preWW2, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }
\NormalTok{lreg12 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg5.postWW2, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }
\NormalTok{lreg13 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg7.preWW2, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }
\NormalTok{lreg14 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(crisisST ~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =} \NormalTok{data_lreg7.postWW2, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# here, grep() returns everything but the 'pattern' from names(coef(lregX))}
\CommentTok{# names(coef(lregX)) returns the names of the coefficients in the respective model}
\NormalTok{exclude =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{grep}\NormalTok{(}\DataTypeTok{pattern =} \StringTok{"dlloansr.lag"}\NormalTok{, }\KeywordTok{names}\NormalTok{(}\KeywordTok{coef}\NormalTok{(lreg11)), }
                 \DataTypeTok{value =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{invert =} \OtherTok{TRUE}\NormalTok{))}

\CommentTok{# so far, we have a vector of strings. 'omit.doef' expects one string, so we}
\CommentTok{# collapse the strings into one string separated by '|'}
\NormalTok{exclude_era =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(exclude, }\DataTypeTok{collapse =} \StringTok{"|"}\NormalTok{)}

\CommentTok{# feel free to uncomment the next line to see which actual coefs get excluded}
\CommentTok{# exclude_logit}

\CommentTok{# adapt model names}
\NormalTok{modelnames_era =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Loans E1 (11)"}\NormalTok{, }\StringTok{"Loans E2 (12)"}\NormalTok{, }\StringTok{"Money E1 (13)"}\NormalTok{, }\StringTok{"Money E2 (14)"}\NormalTok{)}
\CommentTok{# show regression results}
\KeywordTok{showreg}\NormalTok{(}\KeywordTok{list}\NormalTok{(lreg11, lreg12, lreg13, lreg14), }\DataTypeTok{robust =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{robust.type =} \StringTok{"HC4m"}\NormalTok{, }
        \DataTypeTok{digits =} \DecValTok{3}\NormalTok{, }\DataTypeTok{omit.coef =} \NormalTok{exclude_era, }\DataTypeTok{custom.model.names =} \NormalTok{modelnames_era, }
        \DataTypeTok{include.aic =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{include.bic = }\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{include.deviance =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\vskip 2em\begin{SAMEPAGE}
\linespread{0.95}
{\small
\begin{verbatim}
## ==========================================================================
##                 Loans E1 (11)  Loans E2 (12)  Money E1 (13)  Money E2 (14)
## --------------------------------------------------------------------------
## dlloansr.lag1      2.249        -0.316                                    
##                   (2.801)       (3.171)                                   
## dlloansr.lag2      7.697 *       8.307 **                                 
##                   (3.758)       (2.734)                                   
## dlloansr.lag3      2.890         2.946                                    
##                   (3.570)       (2.940)                                   
## dlloansr.lag4      2.486         0.755                                    
##                   (1.931)       (2.766)                                   
## dlloansr.lag5      4.260 *      -1.749                                    
##                   (1.940)       (3.330)                                   
## dlmr.lag1                                       -0.227         2.705      
##                                                 (3.343)       (4.730)     
## dlmr.lag2                                        7.393 *       4.719      
##                                                 (3.351)       (4.167)     
## dlmr.lag3                                        4.077         4.060      
##                                                 (3.251)       (3.358)     
## dlmr.lag4                                       -0.249        -0.838      
##                                                 (2.309)       (5.727)     
## dlmr.lag5                                        4.844         0.808      
##                                                 (2.919)       (4.206)     
## --------------------------------------------------------------------------
## Log Likelihood  -106.421       -83.973        -126.154       -86.712      
## Num. obs.            510           762             585           763          
## ==========================================================================
## *** p < 0.001, ** p < 0.01, * p < 0.05
\end{verbatim}
}
\end{SAMEPAGE}

Both results from the specifications with lagged loans growth and
country fixed effects ((11) and (12)) are again significant for the
second order lag coefficient. Schularick and Taylor point out that model
(12) ``is particularly interesting, since the significant and
alternating signs of the first and second lag coefficients in the
postwar period highlight the sign of the second derivative (not the
first) in raising the risk of a crisis'' (p.~1049).

For the model specified with lagged money growth, only the results for
the pre World War 2 era show a significant coefficient for the second
order lag. Schularick and Taylor also point out that model (14) has a
lower AUC value and thus worse predictive abilities. Therefore let us
compute the ROC-curves, too, to compute the AUC values.

\subsubsection*{\sf \textbf{Predictive power in the pre World War 2
era}}\label{b-predictive-power-in-the-pre-world-war-2-era}

Note here that Schularick and Taylor reported two slightly different
AUC-values for each model. The reason for this is that they first
reported the AUC directly from the models, and then, to compare whether
the money or credit growth models performed better in the respective
eras, compared the results on common samples. Hence in the following we
are just going to compute the ROC curves based on the common samples.

To create the common samples, we extract the fitted values from the
respective models and compare them by row names to keep only those
observations which are present in both specifications.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get fitted and original y-values from model 11 and extract row names}
\NormalTok{fv11 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(lreg11$fitted.values, lreg11$model[}\DecValTok{1}\NormalTok{])}
\NormalTok{fv11$row =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{row.names}\NormalTok{(fv11))}

\CommentTok{# get fitted values from model 13 and extract row names}
\NormalTok{fv13 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(lreg13$fitted.values, lreg13$model[}\DecValTok{1}\NormalTok{])}
\NormalTok{fv13$row =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{row.names}\NormalTok{(fv13))}

\CommentTok{# merge by row names}
\NormalTok{roccomp_pre =}\StringTok{ }\KeywordTok{merge}\NormalTok{(fv11, fv13, }\DataTypeTok{by =} \StringTok{"row"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next we compute the ROC curves. This time we are additionally plotting
the curves together in one figure. Then we will test against the null
hypothesis that the true difference in the areas of the respective
curves is equal to zero.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pROC)}
\CommentTok{# compute the ROC curve}
\NormalTok{loans.ROC_pre =}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =} \NormalTok{roccomp_pre$crisisST.x, }
                   \DataTypeTok{predictor =} \NormalTok{roccomp_pre$lreg11.fitted.values, }
                   \DataTypeTok{auc =} \NormalTok{T, }\DataTypeTok{ci =} \NormalTok{T)}
\CommentTok{# plot ROC curve in red without printing the AUC value}
\KeywordTok{plot}\NormalTok{(loans.ROC_pre, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Call:
## roc.default(response = roccomp_pre$crisisST.x, 
##			   predictor = roccomp_pre$lreg11.fitted.values,     
## 			   auc = T, ci = T)
## 
## Data: roccomp_pre$lreg11.fitted.values in 455 controls 
## (roccomp_pre$crisisST.x 0) < 31 cases (roccomp_pre$crisisST.x 1).
## 
## Area under the curve: 0.7665
## 95% CI: 0.6881-0.8448 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{money.ROC_pre =}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =} \NormalTok{roccomp_pre$crisisST.y, }
                   \DataTypeTok{predictor =} \NormalTok{roccomp_pre$lreg13.fitted.values, }
                   \DataTypeTok{auc =} \NormalTok{T, }\DataTypeTok{ci =} \NormalTok{T)}

\CommentTok{# add the second ROC curve in blue without printing the AUC value}
\KeywordTok{plot}\NormalTok{(money.ROC_pre, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col=} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\IN{-3} \includegraphics{creditboomsgonebust_output_solution_files/figure-latex/3_5__96-1.pdf}

{\small
\begin{verbatim}
## Call:
## roc.default(response = roccomp_pre$crisisST.y, 
##			   predictor = roccomp_pre$lreg13.fitted.values,
##		       auc = T, ci = T)
## 
## Data: roccomp_pre$lreg13.fitted.values in 455 controls 
## (roccomp_pre$crisisST.y 0) < 31 cases (roccomp_pre$crisisST.y 1).
## 
## Area under the curve: 0.7392
## 95% CI: 0.6628-0.8156 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show confidence intervals}
\NormalTok{money.ROC_pre$ci}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## 95% CI: 0.6628-0.8156 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loans.ROC_pre$ci}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## 95% CI: 0.6881-0.8448 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test for difference}
\KeywordTok{roc.test}\NormalTok{(loans.ROC_pre, money.ROC_pre)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##  DeLong's test for two correlated ROC curves
## 
## data:  loans.ROC_pre and money.ROC_pre
## Z = 0.8795, p-value = 0.3791
## alternative hypothesis: true difference in AUC is not equal to 0
## sample estimates:
## AUC of roc1 AUC of roc2 
##   0.7664658   0.7392414
\end{verbatim}
}

The plot of the curves show that there is not that much of a difference
between them, and the test for the difference in the curve confirms this
with a p-value of 0.38, which means there is no statistical evidence for
a difference between these curves. Also, both curves are significantly
different from 0.5 (recall a model with AUC = 0.5 would be a model with
no predictive abilities).

\subsubsection*{\sf \textbf{Predictive power in the post World War 2
era}}\label{b-predictive-power-in-the-post-world-war-2-era}

To compare the models within the time after the Second World War we have
to repeat the procedure one more time. Again we use common samples, thus
we combine the predictions from model (12) and (14) but use only
predictions for those years and countries which are available from both
models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get fitted values from model 12 and extract row names}
\NormalTok{fv12 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(lreg12$fitted.values, lreg12$model[}\DecValTok{1}\NormalTok{])}
\NormalTok{fv12$row =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{row.names}\NormalTok{(fv12))}

\CommentTok{# get fitted values from model 14 and extract row names}
\NormalTok{fv14 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(lreg14$fitted.values, lreg14$model[}\DecValTok{1}\NormalTok{])}
\NormalTok{fv14$row =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{row.names}\NormalTok{(fv14))}

\CommentTok{# merge by row names}
\NormalTok{roccomp_post =}\StringTok{ }\KeywordTok{merge}\NormalTok{(fv12, fv14, }\DataTypeTok{by =} \StringTok{"row"}\NormalTok{)}

\CommentTok{# show that the procedure has filtered out some values}
\KeywordTok{dim}\NormalTok{(fv12)[}\DecValTok{1}\NormalTok{]; }\KeywordTok{dim}\NormalTok{(fv14)[}\DecValTok{1}\NormalTok{]; }\KeywordTok{dim}\NormalTok{(roccomp_post)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## [1] 762

## [1] 763

## [1] 755
\end{verbatim}
}

The last row shows the first dimension (the number of observations/rows)
of each element in the code chunk. Some elements have been removed from
either prediction.

In the preceding code chunk, both receiver operating characteristics
have been computed for you. Now go ahead and plot the curves, specifying
the first curve red and the second one blue, just as we did before with
the pre war models. To add the second one to the first plot, use the
parameter `add'. Then test the two ROC objects with the function
\emph{roc.test()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute ROC for loan growth model}
\NormalTok{loans.ROC_post =}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =} \NormalTok{roccomp_post$crisisST.x, }
                   \DataTypeTok{predictor =} \NormalTok{roccomp_post$lreg12.fitted.values, }
                   \DataTypeTok{auc =} \NormalTok{T, }\DataTypeTok{ci =} \NormalTok{T)}

\CommentTok{# compute ROC for broad money growth model}
\NormalTok{money.ROC_post =}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =} \NormalTok{roccomp_post$crisisST.y, }
                   \DataTypeTok{predictor =} \NormalTok{roccomp_post$lreg14.fitted.values, }
                   \DataTypeTok{auc =} \NormalTok{T, }\DataTypeTok{ci =} \NormalTok{T)}

\KeywordTok{plot}\NormalTok{(loans.ROC_post, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
## Call:
## roc.default(response = roccomp_post$crisisST.x, 
##			   predictor = roccomp_post$lreg12.fitted.values,
##		       auc = T, ci = T)
## 
## Data: roccomp_post$lreg12.fitted.values in 735 controls 
## (roccomp_post$crisisST.x 0) < 20 cases (roccomp_post$crisisST.x 1).
##
## Area under the curve: 0.7405
## 95% CI: 0.6152-0.8659 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(money.ROC_post, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col=} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\IN{-2} \includegraphics{creditboomsgonebust_output_solution_files/figure-latex/3_5__98-1.pdf}

{\small
\begin{verbatim}
## Call:
## roc.default(response = roccomp_post$crisisST.y, 
##			   predictor = roccomp_post$lreg14.fitted.values,
##		       auc = T, ci = T)
## 
## Data: roccomp_post$lreg14.fitted.values in 735 controls 
## (roccomp_post$crisisST.y 0) < 20 cases (roccomp_post$crisisST.y 1).
##
## Area under the curve: 0.6824
## 95% CI: 0.5728-0.7919 (DeLong)
\end{verbatim}
}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{roc.test}\NormalTok{(loans.ROC_post, money.ROC_post)}
\end{Highlighting}
\end{Shaded}

{\small
\begin{verbatim}
##  DeLong's test for two correlated ROC curves
## 
## data:  loans.ROC_post and money.ROC_post
## Z = 1.1812, p-value = 0.2375
## alternative hypothesis: true difference in AUC is not equal to 0
## sample estimates:
## AUC of roc1 AUC of roc2 
##   0.7405442   0.6823810
\end{verbatim}
}

Judging from the plot, the red curve, the ROC curve for model (12),
performs better than the blue one. Consequently, the AUC for model (12)
is with 0.741 higher than the AUC of model (14), which has an AUC of
0.682. This indicates that the model predicting crises based on past
loans growth has a higher predictive power than the model predicting
crises based on past money growth. This would be in line with the fact
that the latter model did not give us significant coefficients. On the
other hand, the test has a p-value of 0.2375 and thus cannot reject the
null hypothesis that there is statistical evidence for a difference
between the curves. Nevertheless the lack of significance and the lower
log likelihood value as well as the lower value of the area under the
ROC curve rather indicate to use lagged values of growth of total bank
loans to predict financial crises.

\eject
}
%\rmfamily
\section{Concluding Remarks}
We have set ourselves two main objectives in the introduction:

\begin{enumerate}
\item An accurate replication of the Stata code accompanying Schularick and Taylor’s paper.
\item The creation of a guided tour through the papers main findings and the means and methods by which the authors have obtained them from their substantial data set.
\end{enumerate}

Overall, it has turned out that the original code could be translated to R without fundamental difficulties. Schularick and Taylor’s data set, consisting of subsets for 14 developed countries, calls for the manipulation of grouped data, which in our opinion is somewhat better supported in Stata than in R. Nevertheless, the package ‘dplyr’ by Wickham and Francois (2015) has turned out to be of great value in this respect. Many native Stata operations could be replicated by functions supplied by this and other packages, some have been provided by ourselves. A first approach, based on the package ‘plm’, although designed specifically for data structures of the type needed here, was less helpful and eventually discarded completely.

In the course of transferring the Stata code into R it was discovered that the authors did make an algorithmic error in calculating the impacts of crises onto the following years’ economic data. This mistake however does not invalidate their findings severely. 

The data set on which the paper is based, in our opinion also is not completely without problems. There are for example undocumented switches between currencies and the number of decimal places (e.g. million vs billion dollars) within one variable with no documentation of this at all, which prohibits direct comparisons of the actual values, and also of other direct aggregates like mean values which are not based on growth rates or ratios of variables. Those kind of aggregates were however presented by Schularick and Taylor. Due to the mentioned reason, we have abstained from presenting them as well. 

Furthermore, some data are clearly faulty (for example, the Great Depression is not indicated as a crisis in England) and some statements in the authors’ explanations do not go with the underlying data set (Schularick and Taylor state “In the prewar sample NLD is dropped […] because there are no crises in the sample” (p. 1049), but in fact their data set shows crises in the Netherlands in 1893, 1907, 1921, and 1939).

In our guided tour we have tried to carve out the computational bases of the answers Schularick and Taylor give to their questions stated in the introduction. Some of their results are obviously convincing and need no further elaboration. For example, the finding that total loan growth was far less influenced by crises in the after World War 2 era underpins the idea that policy changes prevented bank runs and thus proves that lessons from the Great Depression have been learned. 

Although the approach to predict a crisis seems at first to be a bit heuristic, the crisis prediction method seems to be fairly successful. For example, Schularick and Taylor have dated the financial crisis of 2007 – 2009 for seven countries to 2008, only for the US and England it was dated to 2007. We have tested the conjecture that  a redating of the crisis for these seven countries to the year 2007 would diminish the reliability of their crisis forecast for this case. But in fact the results remained significant and the out-of-sample prediction results got even better with the AUC increasing from 0.646 to 0.722.

In general, we find the computations of the main findings of Schularick and Taylor reliable and meaningful.

The RTutor environment proved to be helpful for creating an integrated combination of the paper’s findings and its underlying computations which we hope will stimulate users to their own study of this interesting topic.

\eject

\section{References}\label{exercise-sources}
\setlength{\parskip}{1em}
\linespread{1.0}
Altman, D. G., \& Bland, J. M. (1994). Statistics Notes: Diagnostic
tests 1: sensitivity and specificity (Vol. 308).

Chatterjee, S., \& Hadi, A. S. (2015). Regression Analysis by Example:
Wiley.

Dell'Ariccia, G., Laeven, L., \& Marquez, R. (2014). Real interest rates, leverage, and bank risk-taking. Journal of Economic Theory, 149(0), 65-99. doi: http://dx.doi.org/10.1016/j.jet.2013.06.002

Demirgüç-Kunt, A., \& Huizinga, H. (2004). Market discipline and deposit
insurance. Journal of Monetary Economics, 51(2), 375-399.

Drukker, D. M. \& Gutierrez, R. (2003). Citing references for Stata's
cluster-correlated robust variance estimates. StataCorp LP. from
\url{http://www.stata.com/support/faqs/statistics/references/}.

Gould, W. (2001). Interpreting the intercept in the fixed-effects model.
from \url{http://www.stata.com/support/faqs/stat/xtreg2}

Gönen, M. (2007). Analyzing Receiver Operating Characteristic Curves
with SAS: SAS Institute.

Hart, M. C., Jonker, J., \& van Zanden, J. L. (1997). A Financial
History of the Netherlands: Cambridge University Press.

Hosmer, D. W., \& Lemeshow, S. (2004). Applied Logistic Regression:
Wiley.

Kranz, S., (2014). RTutor: Creating R problem sets with automatic assement of student's solutions.
  R package version 0.7.1.


Krugman, P. (2010). The third depression. The New York Times, 28, A19.

Lesnoff, M., \& Lancelot, R. (2012). aod: Analysis of Overdispersed Data. R
package version 1.3, URL \url{http://cran.r-project.org/package=aod}

Maddaloni, A., \& Peydro, J. L. (2011). Bank risk-taking, securitization, supervision, and low interest rates: Evidence from the Euro-area and the US lending standards. Review of Financial Studies, 24(6), 2121-2165.

R Core Team (2015). R: R Language Definition. URL:
\url{ftp://155.232.191.133/cran/doc/manuals/r-devel/R-lang.pdf}

Schularick, M., \& Taylor, A. M. (2012a). Credit Booms Gone Bust:
Monetary Policy, Leverage Cycles, and Financial Crises, 1870-2008.
American Economic Review, 102(2), 1029-1061. doi: 10.1257/aer.102.2.1029

Schularick, M., \& Taylor, A. M. (2012b). Webappendix to Credit Booms
Gone Bust: Monetary Policy, Leverage Cycles and Financial Crises,
1870-2008, American Economic Review.

Starr, M. A. (2011). Consequences of Economic Downturn: Beyond the Usual Economics: Palgrave Macmillan.

Wickham, H. (2009). ggplot2: Elegant Graphics for Data Analysis:
Springer.

Wickham, H. \& Francois R. (2015). dplyr: A Grammar of Data Manipulation. R package version 0.4.1., http://CRAN.R-project.org/package=dplyr

Wooldridge, J. M. (2012). Introductory Econometrics: A Modern Approach:
South-Western: Cengage Learning.

Yves Croissant, Giovanni Millo (2008). Panel Data Econometrics in R: The
plm Package. Journal of Statistical Software 27(2). URL
\url{http://www.jstatsoft.org/v27/i02/}.

\eject


\thispagestyle{empty}
\begin{center}
{\large\bf Ehrenwörtliche Erklärung}
\end{center}

\vspace*{1cm}

\noindent
Ich erkläre hiermit ehrenwörtlich, dass ich die vorliegende Arbeit
selbstständig angefertigt habe; die aus fremden Quellen direkt oder indirekt 
übernommenen Gedanken sind als solche kenntlich gemacht. Die Arbeit wurde 
bisher keiner anderen Prüfungsbehörde vorgelegt und auch noch nicht 
veröffentlicht.\\

\noindent
Ich bin mir bewusst, dass eine unwahre Erklärung rechtliche Folgen haben wird.

\vspace{2cm}



\noindent Ulm, den 22.04.2015 \hspace{4cm}\hrulefill\\
\vspace*{0.5cm}
\hspace*{11.0cm} (Unterschrift)

%\newpage \vspace*{2cm} \pagestyle{empty}



\end{document}
